{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# script for “A mechanistic model to link technical specifications of vehicle end-of-life treatment with the potential of closed-loop recycling of post-consumer scrap alloys”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd,xlwt\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import collections\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import cdist\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "#set random seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFA_MOD(MFA_time_series_file,scenario_num):\n",
    "    \"\"\"This module provides data regarding # of vehicles\n",
    "    ## Input variables:\n",
    "    -MFA_time_series_file: a workbook contains data sheets of time series data\n",
    "    -scenario_num: scenario number that reflects different assumptions regarding the MFA scenarios\n",
    "    ## Output variables\n",
    "    -num_new_vehicles_dict: a dict that contains the number of new vehicles to be manufactured by category each year, in the format of {\"vehicle type\":[{year1:amount1},{year1:amount1}..]}\n",
    "    -num_retired_vehicles_dict: a dict that contains the number of retired vehicles by category each year, in the format of {\"vehicle type\":[{year1:amount1},{year1:amount1}..]}\n",
    "    -time_span_tuple: a tuple contains the starting and ending years of the period\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \"\"\"load data file\"\"\"\n",
    "    xls_file=pd.ExcelFile(MFA_time_series_file)\n",
    "    #load inflow (new vehicle demand) tabs\n",
    "    inflow_all_sheets=collections.defaultdict()\n",
    "    inflow_all_sheets[1]=pd.read_excel(xls_file,\"consolidated_inflow_s1\").iloc[:,:7] #only keep the columns of interest\n",
    "    inflow_all_sheets[2]=pd.read_excel(xls_file,\"consolidated_inflow_s2\").iloc[:,:7]\n",
    "    inflow_all_sheets[3]=pd.read_excel(xls_file,\"consolidated_inflow_s3\").iloc[:,:7]\n",
    "    #load outflow (vehicle retirement) tabs\n",
    "    outflow_all_sheets=collections.defaultdict()\n",
    "    outflow_all_sheets[1]=pd.read_excel(xls_file,\"consolidated_outflow_s1\").iloc[:,:7]\n",
    "    outflow_all_sheets[2]=pd.read_excel(xls_file,\"consolidated_outflow_s2\").iloc[:,:7]\n",
    "    outflow_all_sheets[3]=pd.read_excel(xls_file,\"consolidated_outflow_s3\").iloc[:,:7]\n",
    "    \n",
    "    \n",
    "    \"\"\"assign data to vehicle demand and retirement\"\"\"\n",
    "    ##create output variables\n",
    "    num_new_vehicles_dict=collections.defaultdict(list)\n",
    "    num_retired_vehicles_dict=collections.defaultdict(list)\n",
    "    time_span_tuple=(int(inflow_all_sheets[2].iloc[0,0]),int(inflow_all_sheets[2].iloc[-1,0]))\n",
    "    \n",
    "    ##assignment based on the scenario specified by the user\n",
    "    for col_name in inflow_all_sheets[scenario_num].columns.values[1:]:\n",
    "        for index,year in enumerate (range (time_span_tuple[0],time_span_tuple[1]+1)):\n",
    "            num_new_vehicles_dict[col_name].append({year:inflow_all_sheets[scenario_num].loc[index,col_name]})\n",
    "            num_retired_vehicles_dict[col_name].append({year:outflow_all_sheets[scenario_num].loc[index,col_name]})                                  \n",
    "    \n",
    "\n",
    "    return time_span_tuple,num_new_vehicles_dict,num_retired_vehicles_dict\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dismantling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dismantling (v_c_tensor,v_s_tensor,mat_compnt_tensor):\n",
    "    \"\"\"This function calculates material composition (by COMPONENT) after dismantling\n",
    "        **this version of the function does tensor operation which 'carries' the third dismension of vehicle type\n",
    "    ## Input variables:\n",
    "    -v_c_tensor: a 3D tensor contains vectors containing mass of the components (j) of a specific type of vehicle (1) among c vehicle types, in total mass (kg), (j,1,c)\n",
    "    -v_s_tensor: a 3D tensor contains vectors containing the dismantling rate of each component (rest is left to \"car hulk\"), corresponding to v_c_tensor, in %, (j,1,c)\n",
    "        note that \"reuse/remanufacturing\" (e.g., remanu of engine) is not considered; \"dismantling rate\" strictly refers to the portion that is \"unwanted and shredded separately (from other components or hulk)\"\n",
    "    -mat_compnt_tensor: materials (row) by components (col) matrix that shows the material composition of each component, \n",
    "       in normalized mass (kg/kg), (n,j), for all vehicle types (c), (n,j,c)\n",
    "    ## Output variables:\n",
    "    -scrap_compnt_tensor: scrap materials (n) by component (j) matrix for vehicle types (c) that shows the material composition of each component for a given vehicle type\n",
    "       This differs from \"mat_com_tensor\", as it is in total mass (kg), (n,j,c)\n",
    "    -scrap_compnt_to_hulk_tensor: scrap materials (n) by component (j) that are not dismantled (will go to car hulk) for each vehicle type (c), in total mass (kg), (n,j,c)    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"Calculate the mass of component that is dismantled (the rest is left with hulk)\"\"\"\n",
    "    ##element-wise multiplication to calculate the mass of compnt that will be shreded separately after dismantling or to be shredded as a whole (hulk)\n",
    "    vc_vs_Hadamard_to_separate_shred=np.multiply(v_c_tensor,v_s_tensor)\n",
    "    vc_vs_Hadamard_to_hulk_shred=np.multiply(v_c_tensor,1-v_s_tensor)\n",
    "    ##diagonalize the results above\n",
    "    #as np.diagnolize is too tricky to use for 3D tensor, I did the following instead\n",
    "    #for those go to separate compnt shredding first\n",
    "    vc_vs_Hadamard_to_separate_shred_diag=np.repeat(vc_vs_Hadamard_to_separate_shred[:,:,:],vc_vs_Hadamard_to_separate_shred.shape[0],axis=1)\n",
    "    eye_matrix=np.eye(vc_vs_Hadamard_to_separate_shred_diag.shape[0])\n",
    "    eye_matrix=np.repeat(eye_matrix[:,:,np.newaxis],vc_vs_Hadamard_to_separate_shred_diag.shape[2],axis=2)\n",
    "    eye_tensor=eye_matrix\n",
    "    vc_vs_Hadamard_to_separate_shred_diag=np.multiply(vc_vs_Hadamard_to_separate_shred_diag,eye_tensor)\n",
    "    #for those go as a whole hulk\n",
    "    vc_vs_Hadamard_to_hulk_shred_diag=np.repeat(vc_vs_Hadamard_to_hulk_shred[:,:,:],vc_vs_Hadamard_to_hulk_shred.shape[0],axis=1)\n",
    "    vc_vs_Hadamard_to_hulk_shred_diag=np.multiply(vc_vs_Hadamard_to_hulk_shred_diag,eye_tensor)\n",
    "   \n",
    "    scrap_compnt_tensor=np.einsum('njc,jlc -> njc',mat_compnt_tensor,vc_vs_Hadamard_to_separate_shred_diag)\n",
    "    scrap_compnt_to_hulk_tensor=np.einsum('njc,jlc -> njc',mat_compnt_tensor,vc_vs_Hadamard_to_hulk_shred_diag)\n",
    "    \n",
    "    return scrap_compnt_tensor,scrap_compnt_to_hulk_tensor    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shredding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shredding(to_be_shredded,AE_matrix,shredding_loss_rate,cross_mixing=True):\n",
    "    \"\"\"This function calculates the shredded alloys and their AE composition of each COMPONENT or CAR HULK\n",
    "    ## Input variables\n",
    "    -to_be_shredded: \n",
    "        \"scrap_compnt_matrix\": (1) scrap materials (row) by component (col) matrix that shows the material composition of each component, \n",
    "        in total mass (kg), (n,j) or (2) 3D tensor that has one more dimension (vehicle type) (n,j,c)\n",
    "        OR\n",
    "        \"car hulk\": (1) scrap materials (row) by car hulk (col=1) matrix that shows the material composition of car hulk, \n",
    "        in total mass (kg), (n,1) or (2) 3D tensor that has one more dimension (vehicle type) (n,1,c)\n",
    "    -AE_matrix: alloys (row) by alloying elements (col) matrix that shows the AE composition of each alloy, in %, (n,z)\n",
    "    -shredding_loss_rate: a list contains the shredding loss rate for each component (len=j) or car hulk (len=1)\n",
    "    -cross_mixing: a boolean variable to indicate if cross_mixing is assumed, \"True\" by default\n",
    "    ## Output variables\n",
    "    -shredded_alloys_mix_by_compnt: a 3D tensor contains shredded scrap alloys (row) by AE composition (col), for each component (channel)\n",
    "        in total mass (kg), (n,z,j)\n",
    "    -shred_loss_to_others: a 3D tensor contains shredded scrap alloys (row) by AE composition (col), for each component (channel) that are lost\n",
    "        in total mass (kg), (n,z,j)\n",
    "    -to_be_shredded_copy: a copy of input variable \"to_be_shredded\" that will be used for creating \"contribute_ratio\" in EoL MOD later, (n,j) or (n,j,c)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"use a for loop to obtain shredded_alloys_mix for each component\"\"\"\n",
    "    #initiate the tensor\n",
    "    row,col,channel=(AE_matrix.shape[0],AE_matrix.shape[1],to_be_shredded.shape[1])\n",
    "    shredded_alloys_mix_by_compnt=np.zeros((row,col,channel))\n",
    "    shred_loss_to_others=np.zeros((row,col,channel))\n",
    "    \n",
    "\n",
    "    ##make a copy of \"to_be_shredded\"\n",
    "    to_be_shredded_copy=deepcopy(to_be_shredded)\n",
    "\n",
    "\n",
    "    ##check if cross-mixing is assumed:\n",
    "    if cross_mixing:\n",
    "        ##collapse the channel dimension (vehicle)\n",
    "        to_be_shredded=np.sum(to_be_shredded,axis=2) #(n,j)        \n",
    "        \n",
    "    \n",
    "    ##shredding\n",
    "    for j in range(to_be_shredded.shape[1]):\n",
    "        #first diagonalize the scrap alloy mix for each component, then dot product is with AE_matrix to get alloy by AE composition for this compnent\n",
    "        temp=np.dot(np.diag(to_be_shredded[:,j]),AE_matrix) \n",
    "        shredded_alloys_mix_by_compnt[:,:,j]=temp*(1-shredding_loss_rate[j])\n",
    "        shred_loss_to_others[:,:,j]=temp*shredding_loss_rate[j]\n",
    "\n",
    "            \n",
    "    return shredded_alloys_mix_by_compnt,shred_loss_to_others,to_be_shredded_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting&remelting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sorting_and_remelting(to_be_sorted,alloy_group_cutoff,TD_matrix,rand_sort_error=0.02):\n",
    "    \"\"\"This function calculates the intermediate scrap alloy by metal group and their AE composition of each COMPONENT \n",
    "    ## Input variables\n",
    "    -to_be_sorted:\n",
    "        shredded_alloys_mix_by_compnt: a 3D tensor contains shredded scrap alloys (row) by AE composition (col), \n",
    "            for each component OR car hulk (channel), in total mass (kg), (n,z,j)\n",
    "    -TD_matrix: thermodynmic limit matrix that contains the yield of AE (col) during remelting of scrap allys of same group (row),\n",
    "        in %, (y,z)\n",
    "    -rand_sort_error: a random sorting error rate (scalar) that indicates the degree of imperfect sorting \n",
    "    -alloy_group_cutoff: a list contains tuple of (staring,ending) rows in the alloy-related matrix that belongs to a certain common metal group,\n",
    "        len()= #of metal types = y\n",
    "    ## Output variables\n",
    "    -IntAlloy_by_compnt: a 3D tensor that contains intermediate scrap alloy by metal group (row), by AE composition (col), \n",
    "        for each component OR car hulk (channel), in total mass (kg), (y,z,j)\n",
    "    -remelt_loss_to_others: a 3D tensor that contains input alloys that are LOST during remelting by metal group (row), by AE composition (col), \n",
    "        for each component OR car hulk (channel), in total mass (kg), (y,z,j)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \"\"\"sort (and combine) scrap alloys by metal group\"\"\"\n",
    "\n",
    "    ##initiate a \"to_be_remelted\" tensor of (y,z,j)\n",
    "    row,col,channel=(TD_matrix.shape[0],TD_matrix.shape[1],to_be_sorted.shape[2])\n",
    "    to_be_remelted=np.zeros((row,col,channel))\n",
    "    #print (\"shape of remelted: \", to_be_remelted.shape)\n",
    "    \n",
    "    for index,cutoff_tuple in enumerate(alloy_group_cutoff):\n",
    "        to_be_remelted[index,:,:]=np.sum(to_be_sorted[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:],axis=0,keepdims=True) #collapse k rows to one row, while keep the other two dimeions unchanged\n",
    "        \n",
    "    if rand_sort_error>0.0:\n",
    "        \"\"\"rand_sort_error algorithm (assume equal probability for all alloy groups)\"\"\"\n",
    "        ##First, set aside the portion of scrap alloy that may be mis-classified\n",
    "        rand_set_aside=to_be_remelted*rand_sort_error #a 3D tensor, if error=0, then no mass is set aside\n",
    "        to_be_remelted-=rand_set_aside\n",
    "\n",
    "        ##create concentration tensor for the set-aside mass tensor\n",
    "        rand_set_aside_conc=np.nan_to_num(rand_set_aside/np.sum(rand_set_aside,axis=1,keepdims=True))\n",
    "        \n",
    "        ##randomly shuffle the index of ALL metal groups\n",
    "        temp_order_all=np.arange(rand_set_aside.shape[0]) #create a array of row index of rand_set_aside, e.g., (0 corresponds to steel)\n",
    "        temp_add_back=np.zeros(rand_set_aside.shape) #(y,z,j) create a zero tensor for adding back later\n",
    "        temp_add_back_mass=np.zeros((rand_set_aside.shape[0],1,rand_set_aside.shape[2])) #(y,1,j) create a tensor to record the mass of metal groups that are added back\n",
    "        add_back_mass_cap=np.sum(rand_set_aside,axis=1,keepdims=True) #(y,1,j) create a mass cap tensor that limits the total mass (NOT individual AE mass) that can be added back to a given metal group\n",
    "        #zero_tensor_mass_cap=np.zeros(add_back_mass_cap.shape) #(y,1,j) create a zero tensor with the same shape of mass cap\n",
    "        np.random.shuffle(temp_order_all) #randomly shuffle the index, so steel (index=0) is not necessarily the first in the order array\n",
    "        #convert array to list\n",
    "        temp_order_all=temp_order_all.tolist()\n",
    "        \n",
    "        ##loop through randomly shuffled index of ALL metal groups:        \n",
    "        for index1 in temp_order_all: \n",
    "            #if the CURRENT metal group does not have any set-aside mass (e.g., no Mg is used in engine), then continue to next iteration\n",
    "            if np.sum(rand_set_aside[index1,:,:])==0: #this means no such metal group is ever used in ANY of the components (otherwise you should still do the following code)\n",
    "                continue\n",
    "            else:\n",
    "                #randomly shuffle the index of set-aside mass of OTHER metal groups\n",
    "                temp_order_other=deepcopy(temp_order_all)\n",
    "                temp_order_other.remove(index1) #remove the index of CURRENT metal group first\n",
    "                np.random.shuffle(temp_order_other) \n",
    "                #append the index of CURRENT metal to the end, in case when you have used up mass of all OTHER metal groups\n",
    "                temp_order_other.append(index1)\n",
    "                \n",
    "                for index2 in temp_order_other:\n",
    "                    if np.sum(temp_add_back[index1,:,:]) < np.sum(add_back_mass_cap[index1,:,:]): #as long as there is one component that needs more mass to be added back, do the following\n",
    "                        #calculate the appropriate mass to add back first (1,j)\n",
    "                        temp_to_add_mass=np.minimum(add_back_mass_cap[index1,:,:]-np.sum(temp_add_back[index1,:,:],axis=0,keepdims=True)\n",
    "                                                             ,np.sum(rand_set_aside[index2,:,:],axis=0,keepdims=True)) #attention: axis is NOT 1, because you've lost the row (index1 or index2) dimension\n",
    "                        temp_add_back_mass[index1,:,:]+=temp_to_add_mass\n",
    "                        #then calculate the AE mass to add back (z,j)\n",
    "                        temp_to_add_AE=np.multiply(temp_to_add_mass,rand_set_aside_conc[index2,:,:]) #(z,j)\n",
    "                        temp_add_back[index1,:,:]+=temp_to_add_AE\n",
    "                        #update corresponding AE mass in rand_set_aside[index2,:,:]\n",
    "                        rand_set_aside[index2,:,:]-=temp_to_add_AE\n",
    "                    else:\n",
    "                        break # if the add_back value for CURRENT metal group is met, no need to loop through the rest of the OTHER metals\n",
    "                        \n",
    "                \n",
    "        ##add back\n",
    "        to_be_remelted+=temp_add_back\n",
    "\n",
    "    #keep a copy of sorted scraps before remelting: used for \"back-calculating\" scraps that need not to be remelted (due to re-balancing)\n",
    "    to_be_remelted_copy=deepcopy(to_be_remelted)\n",
    "    \n",
    "    \"\"\"remelting (explicitly consider thermodynamic limits)\"\"\"\n",
    "    ##initiate \"IntAlloy_by_compnt\" and \"slag_loss_to_others\"\n",
    "    IntAlloy_by_compnt=np.zeros((row,col,channel))\n",
    "    remelt_loss_to_others=np.zeros((row,col,channel))\n",
    "    \n",
    "    ##Calculate the resulting intermediate scrap alloy by metal group, and lost during remelting\n",
    "    for j in range(to_be_remelted.shape[2]):\n",
    "        IntAlloy_by_compnt[:,:,j]=np.multiply(to_be_remelted[:,:,j],TD_matrix)\n",
    "        remelt_loss_to_others[:,:,j]=to_be_remelted[:,:,j]-IntAlloy_by_compnt[:,:,j]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return IntAlloy_by_compnt,remelt_loss_to_others,to_be_remelted_copy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization-based algorithm for Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilution_AddAE_rebal_combined(Int_Target_AE_alloc_by_compnt_ini,AE_matrix,TargetAlloy_mass_by_compnt,\n",
    "                                  ingot_alloy_loss_matrix,alpha,num_iter,epsilon,total_utility_score_goal,alloy_group_cutoff):\n",
    "    \"\"\"this function is called inside the Blending_func when dilution is explicitly considered; this function uses \n",
    "        initial allocation of intermediate scrap alloys and AE spec to generate the mass of virgin bulk metal and\n",
    "        AEs (collectively labeled as 'AE' in this function) needed, as well as the actual mass of intermediate scrap \n",
    "        alloys that are utilized\n",
    "        \n",
    "    **this function combines the \"dilution\", “AE addition” and “re-balancing” steps into one**\n",
    "    **this function DOES NOT distinguish between 'bulk metal' and 'AEs', everything is labeled as 'AE'**\n",
    "    **this function aims to minimize the use of virgin materials\n",
    "    \n",
    "    ## Input variables:\n",
    "    -Int_Target_AE_alloc_by_compnt_ini: a 3d tensor with INITIAL offset of target alloys by intermediate scrap alloys(row),\n",
    "        by AE mass (col),for each component OR car hulk (channel), in kg, (nc,z,j)\n",
    "    -AE_matrix: target alloys (row) by alloying elements (col) matrix that shows the AE composition of each component, in %, (nc,z) \n",
    "    -TargetAlloy_mass_by_compnt:a 3D tensor that contains target alloy by metal (row), by AE composition (col), \n",
    "        for each component OR car hulk (channel), in total mass (kg), (nc,z,j)\n",
    "    -ingot_alloy_loss_matrix: a matrix of alloys from secondary (row) and corresponding cumulative loss from 'ingot' stage to 'alloy' stage, (nc,1)\n",
    "    -alpha: learning rate, scalar (dim=0)\n",
    "    -num_iter: number of iterations before the gradient descent algorithm is terminated, scalar (dim=0)\n",
    "    -epsilon: a small value to create margins of AE_spec%, in %\n",
    "    -total_utility_score_goal: a small value to compare with total utility score of an iteration as the termination criteron , scalar (dim=0)\n",
    "    -alloy_group_cutoff: a list contains tuple of (staring,ending) rows in the alloy-related matrix that belongs to a certain common metal group,\n",
    "        len()= #of metal types\n",
    "        \n",
    "    ## Output variables:\n",
    "    -output_Int_Target_mass_alloc_by_compnt_updated: a 3d tensor with UPDATED offset of target alloys by intermediate scrap alloys(row),\n",
    "        by total mass (col=1),for each component OR car hulk (channel), in kg, (nc,1,j)\n",
    "    -output_virgin_AE_mass_by_compnt: a 3d tensor with mass of virgin AEs (including bulk) (row=1) needed for blending\n",
    "        by AE type (col),for each component OR car hulk (channel), in kg, (1,z,j)\n",
    "    -output_virgin_alloy_needed_by_compnt:a 3d tensor that stores the mass (col=1) of each target alloy from virgin sources(row),\n",
    "        for each component OR car hulk (channel), in total mass (nc,1,j) \n",
    "    -output_virgin_alloy_offset_by_compnt: a 3d tensor that stores mass (col=1) of each target alloy offset by IntAlloy+AE (row), \n",
    "        for each component OR car hulk (channel), in total mass (nc,1,j)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"Initialize variables\"\"\"\n",
    "    dim_z=AE_matrix.shape[1]\n",
    "    dim_j=TargetAlloy_mass_by_compnt.shape[2]\n",
    "    \n",
    "    ###expand dims\n",
    "    ##expand AE_matrix to a 3d tensor (nc,z,1)\n",
    "    #check if AE_matrix has already been extended to 3d outside the function\n",
    "    if len(AE_matrix.shape)==3:\n",
    "        AE_matrix_by_compnt=AE_matrix\n",
    "    else:\n",
    "        AE_matrix_by_compnt=np.expand_dims(AE_matrix,axis=2)\n",
    "        AE_matrix_by_compnt=np.repeat(AE_matrix_by_compnt,dim_j,axis=2) #(nc,z,j)\n",
    "  \n",
    "    ##expand 'ingot-to-alloy' matrix to a 3d tensor (nc,1,1)\n",
    "    #check if the 'ingot-to-alloy' matrix has already been extended to 3d outside the function\n",
    "    if len(ingot_alloy_loss_matrix.shape)==3:\n",
    "        ingot_alloy_loss_matrix_by_compnt=ingot_alloy_loss_matrix\n",
    "    else:\n",
    "        ingot_alloy_loss_matrix_by_compnt=np.expand_dims(ingot_alloy_loss_matrix,axis=2)\n",
    "        ingot_alloy_loss_matrix_by_compnt=np.repeat(ingot_alloy_loss_matrix_by_compnt,dim_j,axis=2) #(nc,1,j)\n",
    "\n",
    "   \n",
    "    zero_tensor=np.zeros(TargetAlloy_mass_by_compnt.shape) #(nc,z,j)\n",
    "    \n",
    "    ###creat margin of AE_matrix for quicker convergence\n",
    "    AE_matrix_by_compnt_margin_min=AE_matrix_by_compnt*(1-epsilon)\n",
    "    AE_matrix_by_compnt_margin_max=AE_matrix_by_compnt*(1+epsilon)\n",
    "    \n",
    "    ###initial concentration of AE in intermediate scrap alloys\n",
    "    #CAUTION: may contain 'nan's, as you may have 0.0/0.0 for certain AE in the IntAlloy\n",
    "    AE_conc_IntAlloy_ini=np.divide(Int_Target_AE_alloc_by_compnt_ini,np.sum(Int_Target_AE_alloc_by_compnt_ini,axis=1,keepdims=True)) \n",
    "    #so, convert 'nan's to zeros (i.e., the mass of this IntAlloy should be zero)\n",
    "    AE_conc_IntAlloy_ini=np.nan_to_num(AE_conc_IntAlloy_ini)\n",
    "        \n",
    "    ###initialize AE% of resulting (after blending) ingot (nc,z,j)\n",
    "    AE_percent_i_j=AE_conc_IntAlloy_ini\n",
    "    \n",
    "    ###initialize beta\n",
    "    beta=np.abs(AE_percent_i_j-AE_matrix_by_compnt)/AE_matrix_by_compnt\n",
    "\n",
    "    ###update total mass of target INGOT(m_i_target) by considering the 'ingot-to-alloy' loss\n",
    "    m_i_target=np.sum(np.multiply(TargetAlloy_mass_by_compnt,ingot_alloy_loss_matrix_by_compnt),axis=1,keepdims=True) #(nc,1,j)\n",
    "\n",
    "    ###calculate total mass demand for AE, regardless of source (virgin or secondary)\n",
    "    AE_mass_demand_regardless=np.multiply(AE_matrix_by_compnt,m_i_target) #(nc,z,j)\n",
    "\n",
    "    ###initialize actual mass of IntAlloy allocation (IntA_i) tensor (nc,1,j)\n",
    "    Int_Target_mass_alloc_by_compnt_ini=np.sum(Int_Target_AE_alloc_by_compnt_ini,axis=1,keepdims=True)\n",
    "    Int_Target_mass_alloc_by_compnt_updated=Int_Target_mass_alloc_by_compnt_ini\n",
    "\n",
    "    ###adjustment to the initil allocation of IntAlloy to avoid over alloc of AE\n",
    "    ##identify the 'over_alloc_IntAlloy' (too much IntAlloy is allocated, as AE_i_j from IntAlloy > total demand)\n",
    "    #create a copy of initial allocation by AE mass\n",
    "    temp_AE_mass_secondary=np.multiply(AE_conc_IntAlloy_ini,Int_Target_mass_alloc_by_compnt_updated) #(nc,z,j)\n",
    "    #create a filter for the difference between allocated AE mass and total demand of AE mass\n",
    "    over_alloc_IntAlloy_filter=temp_AE_mass_secondary-AE_mass_demand_regardless\n",
    "    \n",
    "    #set the 'temp_AE_mass_secondary_diff' to 'over_alloc_IntAlloy_filter' to represent the difference\n",
    "    temp_AE_mass_secondary_diff=over_alloc_IntAlloy_filter\n",
    "    \n",
    "    #create a pool of adjustment to make \n",
    "    pool_of_adjust=np.nan_to_num(temp_AE_mass_secondary_diff/AE_conc_IntAlloy_ini)\n",
    "    #You will end up with'-1.79769313e+308', the largest possible negative value in python, for some cells because you are dividing by zero (0% AE)\n",
    "    #   this should show '-inf', but after you apply 'np.nan_to_num', '-inf' becomes '-1.79769313e+308'\n",
    "    #   such a gigantic negative value does not make sense (you probably need all the scrap ingots in the world in order to add this amount back to 'under-allocated' rows)\n",
    "\n",
    "    #create a temporary storage for max of pool_of_adjust\n",
    "    temp_adjust_max=np.max(pool_of_adjust,axis=1,keepdims=True) #(nc,1,j)\n",
    "    temp_adjust_max[temp_adjust_max<-1.7e+308]=0.0 #set such a crazy value to zero\n",
    "\n",
    "    \"\"\"update the 'temp_adjust_max' to adjust the negative values of rows that are originally 'under-allocated'\"\"\"\n",
    "    for cutoff_tuple in alloy_group_cutoff:\n",
    "        temp_adjust_select=deepcopy(temp_adjust_max[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:]) #store the rows of metal group, (k,1,j)\n",
    "        #sum up the excessive mass (needed to be removed) and deficit mass (needs to be added)\n",
    "        #if you directly use \"temp_excessive=np.sum(temp_adjust_select[temp_adjust_select>0],axis=0,keepdims=True)\", \n",
    "        #   you will have a shape of (1,), because the selection by boolean mask returns a one-dim array\n",
    "        #also make copies of \"temp_adjust_select\", because the changes you are about to make, using boolean mask, will be 'inplace' (i.e., you don't want to mess up with the original data)\n",
    "        temp_adjust_select_copy_for_excessive,temp_adjust_select_copy_for_deficit=deepcopy(temp_adjust_select),deepcopy(temp_adjust_select)\n",
    "        #update the values in the respective arrays\n",
    "        temp_adjust_select_copy_for_excessive[temp_adjust_select<0]=0.0 #set all negative values to zero for 'excessive' array\n",
    "        temp_adjust_select_copy_for_deficit[temp_adjust_select>0]=0.0 #set all positive values to zero for 'deficit' array\n",
    "        #because '1.79769313e+308' is the largest possible (absolute) value in python, when you have more than one of '-1.79769313e+308', you will get '-inf'\n",
    "        #temp_adjust_select_copy_for_deficit[temp_adjust_select_copy_for_deficit<-1.7e+308]=0.0 #such a crazy large negative value is not meaningful in real life anyway (i.e., you won't be able to reassign this much scrap ingot)\n",
    "        \n",
    "        temp_excessive=np.sum(temp_adjust_select_copy_for_excessive,axis=0,keepdims=True) #(1,1,j)\n",
    "        temp_deficit=np.sum(temp_adjust_select_copy_for_deficit,axis=0,keepdims=True) #(1,1,j)\n",
    "        \n",
    "        #determine the TOTAL mass to be added to rows where original assignment is “under-allocated”\n",
    "        #multiply by -1 converts the mass to negative, so that it will lead to addition later in \"Int_Target_mass_alloc_by_compnt_updated-=temp_adjust_max\"\n",
    "        temp_to_be_reassigned=np.minimum(temp_excessive,np.abs(temp_deficit))*-1 #(1,1,j)\n",
    "        #attribute the temp_to_be_reassigned based on the ratio of rows that need additional scrap ingot\n",
    "        temp_reassign_ratio=np.nan_to_num(np.divide(temp_adjust_select_copy_for_deficit,temp_deficit)) #'temp_adjust_select_copy_for_deficit' should only have negative values now\n",
    "        \n",
    "        #update 'temp_adjust_select': because the original negative values in 'temp_adjust_select' may be more than what's available from 'excessive mass' (that needs to be removed from other rows)\n",
    "        #   so need to replace the original one with calculated one here\n",
    "        temp_adjust_max[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:][temp_adjust_max[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:]<0]=0.0 #set original negative values to zero first\n",
    "        temp_adjust_max[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:]+=np.multiply(temp_to_be_reassigned,temp_reassign_ratio) #add back the new negative values (the positive value rows should be affected)\n",
    "                                                    \n",
    "    \n",
    "    #update the allocation of intermediate scrap alloys\n",
    "    Int_Target_mass_alloc_by_compnt_updated-=temp_adjust_max\n",
    "    #there may be very small negative value, as you did division so there may be rounding errors\n",
    "    Int_Target_mass_alloc_by_compnt_updated[Int_Target_mass_alloc_by_compnt_updated<0.00000000001]=0.0\n",
    " \n",
    "    \n",
    "    ###initialize mass of target INGOT after offset (assume no AE addition is needed, i.e., 1:1 substitution)\n",
    "    \"\"\"caution: you may have after_offset alloy equal to total target demand, when no intermediate scrap alloy is used for offset\n",
    "                which could cause \"nan\" or \"inf\" issue in utility function calculation below\n",
    "    \"\"\"\n",
    "    m_i_after_offset=np.maximum(np.sum(zero_tensor,axis=1,keepdims=True),(m_i_target-Int_Target_mass_alloc_by_compnt_updated)) #(nc,1,j)\n",
    "    \n",
    "    ###accordingly, set initial mass of virgin AE (including both bulk and AE) needed to be added to zero\n",
    "    \"\"\"actually set to a small positive number to help converage;\n",
    "            the value needs to be very small (e.g., 10E-7) otherwise you may have an artifact of less than 100% utilization ratio \n",
    "            for alloys that are easy to converge (i.e., AE% very close) and with a very small demand (e.g., 0.25 kg of Mg per vehicle)\n",
    "    \"\"\"\n",
    "    #delta_AE_i_j=zero_tensor #(nc,z,j)\n",
    "    delta_AE_i_j=np.ones(zero_tensor.shape)*0.0000001 #(nc,z,j)\n",
    "    \n",
    "    \"\"\"reinitiate 'm_i_after_offset' to incorporate the 'delta_AE_i_j', such that both variables are from the same iteration\"\"\"\n",
    "    m_i_after_offset-=np.sum(delta_AE_i_j,axis=1,keepdims=True)\n",
    "    \n",
    "    ###initialize utility score (make sure it's larger than the goal, otherwise 'while loop' won't work)\n",
    "    utility_score_tensor=np.sum(zero_tensor,axis=1,keepdims=True)+total_utility_score_goal*2 #(nc,1,j) \n",
    "    average_utility_score=np.nanmean(utility_score_tensor,axis=(0,2)) #(1,)\n",
    "\n",
    "    ###convert total utility score goal from a SCALAR to a Tensor\n",
    "    total_utility_score_goal_tensor=np.ones(utility_score_tensor.shape)*total_utility_score_goal\n",
    "\n",
    "    \n",
    "    \"\"\"gradient descent (for notation, refer to SI of the manuscript)\"\"\"\n",
    "    \n",
    "    ###initialize counter\n",
    "    iter_counter=0\n",
    "    \n",
    "    while iter_counter<num_iter and np.any(utility_score_tensor>total_utility_score_goal_tensor):\n",
    "\n",
    "        ###common term\n",
    "        common_term_1=(np.multiply(AE_conc_IntAlloy_ini,Int_Target_mass_alloc_by_compnt_updated)+delta_AE_i_j)/(m_i_target-m_i_after_offset)-AE_matrix_by_compnt #(nc,z,j)\n",
    "        #if m_i_target-m_i_after_offset=0, then common_term1 will have 'inf' because the numerator is not zero\n",
    "        #so set 'inf' to zero\n",
    "        common_term_1[np.isinf(common_term_1)]=0\n",
    "        common_term_2=1.0/(dim_z*((m_i_target-m_i_after_offset))) #(nc,1,j)\n",
    "        #similarly, handle 'inf' for common_term_2\n",
    "        common_term_2[np.isinf(common_term_2)]=0\n",
    "        #you may have initial value for virgin_alloy_still_needed that is equal to target, when no scrap alloy is used for offset\n",
    "        common_term_0=1.0/(2*dim_z)\n",
    "\n",
    "        ###utility score\n",
    "        square_term=np.square(common_term_1)\n",
    "        \n",
    "        \"\"\"leave the 'nan' as is in 'utility_score_tensor' and calculate an average score to use for While loop\"\"\"\n",
    "        utility_score_tensor=common_term_0*np.sum(square_term,axis=1,keepdims=True) #(nc,1,j)\n",
    "        #calculate an average score to use for While loop: average over axis=0 (nc), then axis=2 (j)\n",
    "        average_utility_score=np.nanmean(utility_score_tensor,axis=(0,2)) #(1,)\n",
    "                \n",
    "        ###calculate AE% of resulting ingot\n",
    "        AE_percent_i_j=(np.multiply(AE_conc_IntAlloy_ini,Int_Target_mass_alloc_by_compnt_updated)+delta_AE_i_j)/(m_i_target-m_i_after_offset)\n",
    "        #if m_i_target-m_i_after_offset=0, then AE_percent_i_j will have 'inf' because the numerator is not zero\n",
    "        #AE_percent_i_j[np.isinf(AE_percent_i_j)]=0\n",
    "        #print \"AE% after blending for [last alloy (AZ91),:,first compnt] : \", AE_percent_i_j[-1,:,0],\"\\n\"\n",
    "        \n",
    "        ###derivative wrt additional vrigin AE (dU/dTheta_i_j), (nc,1,j)\n",
    "        #CAUTION: this derivative is the same for ALL AE (j), so you will need to have AE-specific adjustment (beta)\n",
    "        dTheta_i_j=common_term_2*np.sum(np.abs(common_term_1),axis=1,keepdims=True)\n",
    "        \n",
    "        ###derive beta\n",
    "        #if the AE% is already within the margin, no need to update Thetas, hence beta=0 (so set all elements to zero first)\n",
    "        beta=np.zeros(beta.shape)\n",
    "        #create a filter to update elements where AE_percent_i_j is NOT within the margin\n",
    "        filter_outside_margin_1=AE_percent_i_j<AE_matrix_by_compnt_margin_min\n",
    "        filter_outside_margin_2=AE_percent_i_j>AE_matrix_by_compnt_margin_max\n",
    "        filter_outside_margin=filter_outside_margin_1 ^ filter_outside_margin_2        \n",
    "                        \n",
    "        beta[filter_outside_margin]=np.abs(AE_percent_i_j[filter_outside_margin]-AE_matrix_by_compnt[filter_outside_margin])/AE_matrix_by_compnt[filter_outside_margin]\n",
    "        beta[np.where(np.isinf(beta))]=1.0\n",
    "        \n",
    "        beta*=alpha\n",
    "        \n",
    "        ###update delta_AE\n",
    "        delta_AE_i_j+=np.multiply(beta,dTheta_i_j)\n",
    "        \n",
    "        #if there is \"nan\" in delta_AE_i_j, then it means \"m_i_target-m_i_after_offset=0\", i.e., after_offset virgin alloy\n",
    "        #is equal to target demand and no scrap alloy is used for offset-->thus no any delta_AE (j) of this alloy (i) is needed to be added\n",
    "        delta_AE_i_j=np.nan_to_num(delta_AE_i_j)\n",
    "        \n",
    "        #calcualte virgin alloy still needed\n",
    "        m_i_after_offset=m_i_target-Int_Target_mass_alloc_by_compnt_updated-np.sum(delta_AE_i_j,axis=1,keepdims=True)\n",
    "        \n",
    "        \n",
    "        \"\"\"to re-assign the removed scrap ingot to other rows\"\"\"\n",
    "        ##Make a copy of ‘m_i_after_offset’ and set all positive values to zero, all negative values to positive\n",
    "        excess_scrap_ingot_available_for_removal=m_i_after_offset.copy() #this creates a deep copy of the array\n",
    "        excess_scrap_ingot_available_for_removal[excess_scrap_ingot_available_for_removal>0]=0.0 #set all positive values (where virgin alloys are still needed after blending) to zero \n",
    "        excess_scrap_ingot_available_for_removal*=-1 #set all negative values to positive (this becomes the postive amount of scrap ingot that can be reassigned)\n",
    "        \n",
    "        ##update the 'Int_Target_mass_alloc_by_compnt_updated', the relevant rows are those correspond to negative values in \"m_i_after_offset\"\n",
    "        #make a copy of 'Int_Target_mass_alloc_by_compnt_updated'\n",
    "        copy_Int_Target_mass_alloc_by_compnt_updated=Int_Target_mass_alloc_by_compnt_updated.copy()\n",
    "        Int_Target_mass_alloc_by_compnt_updated[m_i_after_offset<0]-=excess_scrap_ingot_available_for_removal[m_i_after_offset<0]\n",
    "        #check if any of the corresponding rows in 'Int_Target_mass_alloc_by_compnt_updated' becomes negative or zero\n",
    "        #  if so, it means too much 'delta_AE' is added (i.e., the quality of scrap ingot is too low for recycling for this particular row of targeta alloy)\n",
    "        #so, first adjust delta_AE to set it to zero when scrap ingot assignment is negative or zero\n",
    "        negative_scrap_assign_index=np.where(Int_Target_mass_alloc_by_compnt_updated<=0) #(nc,1,j)\n",
    "        delta_AE_i_j[negative_scrap_assign_index[0],:,negative_scrap_assign_index[2]]=0.0 #as the second dim of crazy_index is 1, need manually set all second dim (z) of delta_AE_i_j to zero\n",
    "        #then, set negative or zero assignment of scrap ingot to zero\n",
    "        Int_Target_mass_alloc_by_compnt_updated[Int_Target_mass_alloc_by_compnt_updated<=0]=0.0\n",
    "        #Update ‘m_i_after_offset’  accordingly\n",
    "        m_i_after_offset=m_i_target-Int_Target_mass_alloc_by_compnt_updated-np.sum(delta_AE_i_j,axis=1,keepdims=True)\n",
    "        #don't forget to update \"excess_scrap_ingot_available_for_removal\": \n",
    "        #  if the mass to be removed is more than originally assigned (due to extremely high AE addition),\n",
    "        #  set the removal equal to the orignial assignment\n",
    "        more_than_original_index=np.where(excess_scrap_ingot_available_for_removal>copy_Int_Target_mass_alloc_by_compnt_updated)\n",
    "        excess_scrap_ingot_available_for_removal[more_than_original_index]=copy_Int_Target_mass_alloc_by_compnt_updated[more_than_original_index]\n",
    "            \n",
    "        ##update the 'Int_Target_mass_alloc_by_compnt_updated' again, to re-assign the removed scrap ingots to those rows where virgin alloy is still needed\n",
    "        for cutoff_tuple in alloy_group_cutoff:\n",
    "            temp_supply_for_reassign_select=deepcopy(excess_scrap_ingot_available_for_removal[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:]) #store a copy of the rows of metal group that can provide removed scrap ingots, (k,1,j)\n",
    "            temp_sink_for_reassign_select=deepcopy(m_i_after_offset[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:]) #'m_i_after_offset' contains the rows where virgin alloys are still used (i.e., may potentially take up removed scrap ingots, (k,1,j)\n",
    "                                                                                                            \n",
    "            #sum up the excessive mass (available for reassignment) and sink mass (can potentially take these excessive mass)\n",
    "            temp_supply_total=np.sum(temp_supply_for_reassign_select,axis=0,keepdims=True) #(1,1,j)\n",
    "            temp_sink_total=np.sum(temp_sink_for_reassign_select,axis=0,keepdims=True) #(1,1,j)\n",
    "        \n",
    "            #determine the TOTAL mass to be added to rows where virgin alloys are still needed after blending\n",
    "            temp_to_be_reassigned_in_loop=np.minimum(temp_supply_total,temp_sink_total) #(1,1,j)\n",
    "\n",
    "            #attribute the temp_to_be_reassigned based on the ratio of rows that need additional scrap ingot\n",
    "            temp_reassign_ratio_in_loop=np.nan_to_num(np.divide(temp_sink_for_reassign_select,temp_sink_total)) #(k,1,j)\n",
    "            \n",
    "            #update 'Int_Target_mass_alloc_by_compnt_updated' with additional scrap ingot use\n",
    "            Int_Target_mass_alloc_by_compnt_updated[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:]+=np.multiply(temp_to_be_reassigned_in_loop,temp_reassign_ratio_in_loop) #add back the new negative values (the positive value rows should be affected)\n",
    "            #Update ‘m_i_after_offset’  accordingly\n",
    "            m_i_after_offset=m_i_target-Int_Target_mass_alloc_by_compnt_updated-np.sum(delta_AE_i_j,axis=1,keepdims=True)\n",
    "   \n",
    "        \n",
    "        ##check mass balance\n",
    "        \"\"\"This function may leave some very small discrepancy in mass balance (e.g., 10E-10)\"\"\"\n",
    "        mass_balance=m_i_after_offset+Int_Target_mass_alloc_by_compnt_updated+np.sum(delta_AE_i_j,axis=1,keepdims=True)-m_i_target\n",
    "\n",
    "        \n",
    "        ###move to next iteration\n",
    "        iter_counter+=1\n",
    "    \n",
    "    \"\"\"check AE mass balance (based on the latest iteration)\"\"\"\n",
    "    AE_mass_balance=np.multiply((Int_Target_mass_alloc_by_compnt_updated+np.sum(delta_AE_i_j,axis=1,keepdims=True)),AE_percent_i_j)+np.multiply(m_i_after_offset,AE_matrix_by_compnt)-np.multiply(m_i_target,AE_matrix_by_compnt) #(nc,z,j)\n",
    "    \n",
    "    temp_AE_balance_for_print=AE_mass_balance.copy()\n",
    "    temp_AE_balance_for_print[np.isinf(AE_mass_balance)]=0.0\n",
    "    print (\"total mass imbalance as a % of total material demand: \", np.sum(abs(np.nan_to_num(temp_AE_balance_for_print)))/np.sum(np.nan_to_num(m_i_target))*100,\"%\",\"\\n\")\n",
    "\n",
    "    #output the AE mass balance result to csv\n",
    "    np.savetxt(\"AE mass balance (should be all zeros)_CSV.csv\", AE_mass_balance[:,:,0], delimiter=\",\")\n",
    "    \n",
    "    ##check iteration numbers and average utility scores\n",
    "    print (\"the End, iter_counter is: \", iter_counter,\"\\n\")  \n",
    "    print (\"final average utility score is: \", average_utility_score, \"\\n\")\n",
    "    \n",
    "    \n",
    "    \"\"\"create output variables and their values\"\"\"\n",
    "    #left_over IntAlloy will be calculated outside this function\n",
    "    output_Int_Target_mass_alloc_by_compnt_updated=Int_Target_mass_alloc_by_compnt_updated #(nc,1,j)\n",
    "    output_virgin_AE_mass_by_compnt=delta_AE_i_j #(nc,z,j)\n",
    "    output_virgin_alloy_needed_by_compnt=np.divide(m_i_after_offset,ingot_alloy_loss_matrix_by_compnt) #(nc,1,j), to convert from 'ingot' needed to 'final alloy' needed\n",
    "    output_virgin_alloy_offset_by_compnt=np.sum(TargetAlloy_mass_by_compnt,axis=1,keepdims=True)-output_virgin_alloy_needed_by_compnt #(nc,1,j)\n",
    "      \n",
    "    \"\"\"output (1) AE% of resulting ingots, (2) difference between resulting ingots and spec, (3) AE_matrix_by_compnt\"\"\"\n",
    "    #[caution]this always output the results for the last vehicle type (i.e., EV light)\n",
    "    np.savetxt(\"AE%_of_resulting_ingots_CSV.csv\", AE_percent_i_j[:,:,0], delimiter=\",\")\n",
    "    np.savetxt('difference_between_blended_and_spec_CSV.csv',AE_percent_i_j[:,:,0]-AE_matrix_by_compnt[:,:,0],delimiter=\",\")\n",
    "    \n",
    "    ##also print out the AE_matrix_by_compnt (should be exactly/very close to AE matrix that is provided exogenously)\n",
    "    #[caution]this always output the results for the last vehicle type (i.e., EV light)\n",
    "    np.savetxt('AE_spec_from_blending_step.csv',AE_matrix_by_compnt[:,:,0],delimiter=\",\")\n",
    "    \n",
    "    return output_Int_Target_mass_alloc_by_compnt_updated,output_virgin_AE_mass_by_compnt,output_virgin_alloy_needed_by_compnt,output_virgin_alloy_offset_by_compnt    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Blending(IntAlloy_by_compnt,TargetAlloy_mass_by_compnt,AE_matrix,alloy_group_cutoff,ingot_alloy_loss_matrix,\n",
    "             alpha,num_iter,epsilon,total_utility_score_goal,closed_loop_recycl_rate_list,\n",
    "             IntAlloy_mass_alloc='Euclidean distance',dilution_explicit=True,one_very_small_value=0.00001,**kwargs):\n",
    "    \"\"\"This function calculates the virgin material use for blending (with intermediate scrap alloys) for different components\n",
    "    ## Input variables\n",
    "    -IntAlloy_by_compnt: a 3D tensor that contains intermediate scrap alloy by metal group (row), by AE composition (col), \n",
    "        for each component OR car hulk (channel), in total mass (kg), (y,z,j)\n",
    "        *if \"Cross-Mixing\", this input should be partial of total (based on the ratio of vehicle demand by type)\n",
    "    -AE_matrix: alloys (row) by alloying elements (col) matrix that shows the AE composition of each component, in %, (n,z) \n",
    "    -TargetAlloy_mass_by_compnt:a 3D tensor that contains target alloy by metal (row), by AE composition (col), \n",
    "        for each component OR car hulk (channel), in total mass (kg), (n,z,j)\n",
    "    -alloy_group_cutoff: a list contains tuple of (staring,ending) rows in the alloy-related matrix that belongs to a certain common metal group,\n",
    "        len()= #of metal types\n",
    "    -ingot_alloy_loss_matrix: a matrix of alloys from secondary (row) and corresponding cumulative loss from 'ingot' stage to 'alloy' stage, (n,1)\n",
    "    -IntAlloy_mass_alloc: method to allocate intermediate scrap alloy for blending for target alloy, in text\n",
    "        \"cosine similarity\": allocate the mass of intermediate scrap alloy (of a certain metal group) based on the order of \n",
    "            how \"close\" the composition is between the scrap alloy and target alloy\n",
    "        Or\n",
    "        \"distribution by target ratio\": allocate the mass of intermediate scrap alloy (of a certain metal group) \n",
    "            based on the (total mass) ratio of target alloys\n",
    "        Or\n",
    "        \"Euclidean distance\": allocate the mass of intermediate scrap alloy (of a certain metal group) \n",
    "            based on the Euclidean distance of AE concentrations between the scrap and target alloys\n",
    "    -dilution_explicit: a boolean variable to indicate whether \"dilution (quality loss)\" is explicitly modeled\n",
    "    -closed_loop_recycl_rate_list: a list containing the metal group-lvl closed-loop recycling rate factors, in %\n",
    "    ***below four are added for the optimization-based dilution function***\n",
    "    -alpha: learning rate, scalar (dim=0)\n",
    "    -num_iter: number of iterations before the gradient descent algorithm is terminated, scalar (dim=0)\n",
    "    -epsilon: a small value to create margins of AE_spec%, in %\n",
    "    -total_utility_score_goal: a small value to compare with total utility score of an iteration as the termination criteron , scalar (dim=0)\n",
    "    ***above four are added for the optimization-based dilution function***\n",
    "    -keyword arguments:\n",
    "        weight_vector: this gives weight for each AE element (z,) during mass allocation of intermediate scrap alloy, e.g., when a certain AE is given a high weight\n",
    "            more of the intermediate scrap alloy may be allocated to produce target alloy between which the difference in AE mass is the smallest\n",
    "            (i.e., less virgin AE of high weight is needed)\n",
    "    ## Output variables\n",
    "    -virgin_bulk_metal_mass_by_compnt: a 3D tensor containing virgin bulk metals (row), by total mass (col), for each component OR car hulk (channel), in total mass (kg), (y,1,j)\n",
    "    -virgin_AE_mass_by_compnt: a 3D tensor containing (1 row of) virgin AEs needed (col), for each component OR car hulk (channel), in total mass (kg), (1,z,j)\n",
    "    -Int_Target_mass_alloc_by_compnt: a 3D tensor that stores the mass of intermediate scrap alloy (col=1) allocated to each target alloy (row),\n",
    "        for each component OR car hulk (channel), in total mass (n,1,j)\n",
    "    -Int_Target_AE_alloc_by_compnt: a 3D tensor that stores the mass of AEs (including bulk) that constitute intermediate scrap alloy (col=z),\n",
    "        allocated to each target alloy (row),for each component OR car hulk (channel), in total mass (n,z,j)\n",
    "    -virgin_alloy_needed_by_compnt: a 3D tensor that stores the mass (col=1) of each target alloy from virgin sources(row),\n",
    "        for each component OR car hulk (channel), in total mass (n,1,j)\n",
    "    -IntAlloy_leftover_by_compnt: a 3D tensor that stores the AE mass (col=z) of each intermediate scrap alloy that is not used up (row),\n",
    "        for each component OR car hulk (channel), in total mass (y,z,j)\n",
    "    -virgin_alloy_offset_by_compnt: a 3D tensor that stores the mass (col=1) of each target alloy from secondry sources (row),\n",
    "        for each component OR car hulk (channel), in total mass (n,1,j)\n",
    "    -IntAlloy_mass_alloc_by_compnt: a 3D tensor that stores the mass (col=1) of each intermediate scrap alloy that is used for blendng(row),\n",
    "        for each component OR car hulk (channel), in total mass (y,1,j)\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    \"\"\"(Initial) intermediate scrap alloy ALLOCATION for target alloy production\"\"\"\n",
    "    ##Initiation\n",
    "    virgin_bulk_metal_mass_by_compnt=np.zeros((IntAlloy_by_compnt.shape[0],1,IntAlloy_by_compnt.shape[2])) #(y,1,j)\n",
    "    virgin_AE_mass_by_compnt=np.zeros((1,IntAlloy_by_compnt.shape[1],IntAlloy_by_compnt.shape[2])) #(1,z,j)\n",
    "    IntAlloy_leftover_by_compnt=np.zeros((IntAlloy_by_compnt.shape[0],IntAlloy_by_compnt.shape[1],IntAlloy_by_compnt.shape[2])) #(y,z,j)\n",
    "    Int_Target_mass_alloc_by_compnt=np.zeros((TargetAlloy_mass_by_compnt.shape[0],1,TargetAlloy_mass_by_compnt.shape[2])) #(n,1,j)\n",
    "    Int_Target_AE_alloc_by_compnt=np.zeros((TargetAlloy_mass_by_compnt.shape[0],TargetAlloy_mass_by_compnt.shape[1],TargetAlloy_mass_by_compnt.shape[2])) #(n,z,j)\n",
    "    virgin_alloy_needed_by_compnt=np.sum(TargetAlloy_mass_by_compnt,axis=1,keepdims=True) #initialize the virgin alloy need to total alloy need (assume no scrap recycling yet)\n",
    "    Int_Target_AE_percent_ini=np.zeros(Int_Target_AE_alloc_by_compnt.shape) #AE% in intially allocated scrap alloys (n,z,j)\n",
    "\n",
    "    \"\"\"adjust target alloy demand to 'target ingot demand' \"\"\"\n",
    "    #expand the dimension of 'ingot_alloy_loss_matrix' from (nc,1) to (nc,1,1)\n",
    "    ingot_alloy_loss_matrix=ingot_alloy_loss_matrix.as_matrix()    \n",
    "    ingot_alloy_loss_matrix=np.expand_dims(ingot_alloy_loss_matrix,axis=2) #expand the dimension to 3d, should be (nc,1,1) now\n",
    "    #make sure \"virgin_alloy_needed_by_compnt\" is not affected\n",
    "    TargetAlloy_mass_by_compnt=np.multiply(TargetAlloy_mass_by_compnt,ingot_alloy_loss_matrix)\n",
    "\n",
    "    ###calculate the AE% in intermediate scrap alloys and target alloys, as they will be used by \"distance-based\" (initial) allocation and Dilution step\n",
    "    ##Target alloy specs first (caution: as not all target alloys will be used in each component)\n",
    "    temp_target_mass_adjust=np.sum(TargetAlloy_mass_by_compnt, axis=2,keepdims=True) #(n,z,1) collapse along component dimension to make sure each alloy has a non-zero value\n",
    "                                                                                        #make sure each target alloy is used by at least one component\n",
    "    \n",
    "\n",
    "    ##assign the matrix (n,z) first\n",
    "    TargetAlloy_AE_percent_spec=AE_matrix\n",
    "    ##then expand to 3D\n",
    "    TargetAlloy_AE_percent_spec=np.repeat(TargetAlloy_AE_percent_spec[:,:,np.newaxis],Int_Target_AE_percent_ini.shape[2],axis=2) #(n,z,j)\n",
    "           \n",
    "    ##Intermediate scrap alloy\n",
    "    #calculate the AE% by metal group first    \n",
    "    Int_Target_AE_percent_ini_by_metal_group=IntAlloy_by_compnt/np.sum(IntAlloy_by_compnt,axis=1,keepdims=True) #(y,z,j)\n",
    "        \n",
    "    #then expand to each target alloy in that metal group\n",
    "    for index, cutoff_tuple in enumerate(alloy_group_cutoff):\n",
    "        Int_Target_AE_percent_ini[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:]=Int_Target_AE_percent_ini_by_metal_group[index,:,:]\n",
    "    \n",
    "    \n",
    "    ###'distribution by target ratio' method is less complicated, as it allocates the mass all at once, \n",
    "    if IntAlloy_mass_alloc=='distribution by target ratio' or dilution_explicit==False:\n",
    "        ##create a 3D tensor to normalize the target alloy (total mass) ratio for all components\n",
    "        for index,cutoff_tuple in enumerate(alloy_group_cutoff):\n",
    "            temp_target_ratio=np.sum(TargetAlloy_mass_by_compnt[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:],axis=1,keepdims=True) #(k,1,j), k rows are (within a metal group) preserved\n",
    "            target_alloy_ratio=temp_target_ratio/np.sum(temp_target_ratio, axis=0,keepdims=True) #(k,1,j)\n",
    "            ##(inital) allocate mass of intermeidate scrap alloy, take ONE row from y rows and spread by multiply with \"target_alloy_ratio\"\n",
    "            #np.einsum here is equal to element-wise multiplication between the two arrays (with broadcasting), because axis \"z\" is retained\n",
    "            Int_Target_mass_alloc_by_compnt[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:]=np.einsum('lzj,kzj->kzj',np.sum(IntAlloy_by_compnt[index:index+1,:,:],axis=1,keepdims=True),target_alloy_ratio) #(k of n,(z=)1,j; l=1)    \n",
    "            #also record the allocation in terms of AE mass\n",
    "            Int_Target_AE_alloc_by_compnt[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:]=np.einsum('lzj,kzj->kzj',IntAlloy_by_compnt[index:index+1,:,:],target_alloy_ratio) #(k of n,z(!=1 for IntAlloy, =1 for ratio),j; l=1) \n",
    "\n",
    "        Int_Target_mass_alloc_by_compnt=np.nan_to_num(Int_Target_mass_alloc_by_compnt)\n",
    "        Int_Target_AE_alloc_by_compnt=np.nan_to_num(Int_Target_AE_alloc_by_compnt)\n",
    "\n",
    "    ##After the lines above, (initial) allocation of mass of intermediate scrap alloys of ALL metal groups and ALL components are assigned##\n",
    "  \n",
    "\n",
    "    ###Distance-based methods share the same data pre-processing steps###\n",
    "    else:\n",
    "        #initiate variables\n",
    "        Int_Target_dict=collections.defaultdict(list)\n",
    "        \n",
    "        try:\n",
    "            weight_vector=kwargs['weight_vector'] #if there is a keyword argument input for weight matri, use it; otherwise, use default\n",
    "        except:\n",
    "            weight_vector=np.ones((IntAlloy_by_compnt.shape[1])) #default setting: all ones for z AEs in 1D array (z,)\n",
    "\n",
    "        #sum up the mass of AEs to get the total mass of intermeidate alloys\n",
    "        IntAlloy_total_mass_by_compnt=np.sum(IntAlloy_by_compnt,axis=1,keepdims=True) #(y,1,j)\n",
    "            \n",
    "        for compnt_index in range(TargetAlloy_mass_by_compnt.shape[2]): #loop through each component channel\n",
    "                                                                        #can't carry the jth dimension in the calculation as cosine takes in 1D array only\n",
    "            for index,cutoff_tuple in enumerate(alloy_group_cutoff): #cutoff for metal groups never change\n",
    "                #update the Int_Target_dict every time you switch to another component channel\n",
    "                Int_Target_dict[index]=TargetAlloy_mass_by_compnt[cutoff_tuple[0]:cutoff_tuple[1]+1,:,compnt_index] #the selected \"portion\" of \"TargetAlloy_mass_by_compnt\" has the dimension of (k,z,)\n",
    "                                                                                                                    #total number of \"index\" is equal to # of metal groups (y)\n",
    "            ##calculate similarity and allocate mass for each metal group   \n",
    "            for IntAlloy_index,target_alloys in Int_Target_dict.items(): #number of rows of IntAlloy is the same as number of rows in cutoff tuple dict\n",
    "\n",
    "                similarity_list=[] #initiate the similarity score as an empty list\n",
    "                ranked_alloy_list=[]#for ranking the target alloys for a given metal group\n",
    "                \n",
    "                ##get the intermediate scrap alloy that will be used for comparison and allocation\n",
    "                #get a total mass that will be REDUCED during each iteration of allocation\n",
    "                temp_IntAlloy_total_mass=IntAlloy_total_mass_by_compnt[IntAlloy_index,:,compnt_index] #dimension=(1,)\n",
    "                #create a copy of total mass that will NOT change during the allocation process\n",
    "                temp_IntAlloy_total_mass_locked=IntAlloy_total_mass_by_compnt[IntAlloy_index,:,compnt_index] #dimension=(1,)\n",
    "                temp_IntAlloy=np.squeeze(IntAlloy_by_compnt[IntAlloy_index,:,compnt_index]) #(z,)\n",
    "                #find the corresponding scrap alloy concentration\n",
    "                temp_IntAlloy_conc=Int_Target_AE_percent_ini_by_metal_group[IntAlloy_index,:,compnt_index] #(z,)\n",
    "        \n",
    "                \"\"\"check for 'nan': skip this run if 'temp_IntAlloy_conc' contains 'nan' \"\"\"\n",
    "                if np.any(np.isnan(temp_IntAlloy_conc)):\n",
    "                    #print \"an 'nan' is found!\"\n",
    "                    #print temp_IntAlloy_conc\n",
    "                    continue\n",
    "                \"\"\"\n",
    "                by 'continue', you are skipping the 'zero mass' scrap alloys and hence avoid comparison of AE composition with target alloys (below)\n",
    "                As this comparison is a 'one (intermediate scrap alloy) to many (target alloys)' problem, skipping one intermediate scrap alloy\n",
    "                will avoid the comparision for all relevant target alloys, which is reasonable as you won't have scrap alloy to make any for these target alloys\n",
    "                \"\"\"\n",
    "            \n",
    "                #loop through targe alloys (k,z) of a given metal group , similarity=1-cosine distance\n",
    "                for alloy in target_alloys: #by default takes each row (z,) of a tensor (k,z,)\n",
    "                    temp_target=np.squeeze(alloy) #just as a precaution to make sure dimension =(z,)\n",
    "                    #create a concentration version of temp_target\n",
    "                    #caution: to avoid the situation where the concentrations are very similar between scrap and target\n",
    "                    #but no demand of such target of a given component (i.e., allocatin should be zero)\n",
    "                    if np.sum(temp_target)!=0: #this also helps to avoid the \"division by zero\" issue\n",
    "                        temp_target_conc=temp_target/np.sum(temp_target,keepdims=True) #(z,)\n",
    "\n",
    "                        ##calculate the similarity score and use it to rank the target alloy (array of mass) accordingly\n",
    "                        ##placing specific method check here can be redundant, as within each component, you have to check it once, NOT GOOD##\n",
    "\n",
    "                        ###'cosine similarity' method is more complicated, as each time a closest target alloy is identified,\n",
    "                        ### then after the corresponding mass is allocated to meet the demand of that target alloy,\n",
    "                        ### then the next closest target alloy is identified   \n",
    "                        if IntAlloy_mass_alloc=='cosine similarity':\n",
    "                            similarity_list.append(1-cosine(temp_IntAlloy_conc,temp_target_conc,w=weight_vector))\n",
    "                        ###'Euclidean distance' method \n",
    "                        elif IntAlloy_mass_alloc=='Euclidean distance':\n",
    "                            #need to increase the dimension to a 2D-array for Euclidean distance...\n",
    "                            temp_shape=temp_target_conc.shape[0] #DO use temp_target_conc.shape[0] because it gets new input (z,) everytime\n",
    "                            temp_IntAlloy_conc=temp_IntAlloy_conc.reshape((1,temp_shape)) #(1,z); this is actually redundant, as after 1st iteration, the dimension is already (1,z)\n",
    "                            temp_target_conc=temp_target_conc.reshape((1,temp_shape)) #(1,z)\n",
    "                            #as the target alloys will re-ordered by be similarity score (the higher the better)\n",
    "                            #this means for Euclidean distance, the larger distance needs to transform to a lower score;\n",
    "                            #therefore, take negative of the Euclidean distance as similarity score\n",
    "                            similarity_list.append(-1*cdist(temp_IntAlloy_conc,temp_target_conc,w=weight_vector).tolist()[0][0]) #flatten the list, just as an precaution\n",
    "                    else:\n",
    "                        similarity_list.append(-99999) #assign very large negative value to put the \"no-demand\" alloys to the end of the list\n",
    "                    #add results to the lists first (matching the index between 'similarity' and 'alloy' lists)\n",
    "                    ranked_alloy_list.append(np.sum(alloy)) #each 'alloy' from (z,) to (1,)\n",
    "            \n",
    "                #then get the sorted index of the similartiy_list (for similarity value sorted from large to small)\n",
    "                temp_new_order=np.argsort(similarity_list).tolist()\n",
    "                temp_new_order.reverse() #this reverse the order of sorted index, such that higher similarity value ranks higher\n",
    "                                        #as list.reverse() modifies the list 'inplace' and returns None, if you use chain command,\n",
    "                                        #'temp_new_order' will be assigned None (returned by the last command 'list.reverse()')\n",
    "                \n",
    "                ##allocate the mass to target alloy (of a given metal group)                \n",
    "                ##(linear) method\n",
    "                #prepare a list of index that corresponds to the row numbers in the entire \"Int_Target_mass_alloc_by_compnt\" tensor \n",
    "                #    (should have the same dimension as other 3D tensors such as \"virgin_alloy_needed_by_compnt\")\n",
    "                cutoff_tuple=alloy_group_cutoff[IntAlloy_index] #get the cutoff_tuple corresponding to this metal group\n",
    "                global_row_index_of_k=[index for index in range(cutoff_tuple[0],cutoff_tuple[1]+1)] #means the global row index representation of the index of k rows for this loop\n",
    "                                \n",
    "                for sorted_index in temp_new_order: #start from the most similar alloy\n",
    "                    if temp_IntAlloy_total_mass>0: #check if there is still mass left for making this specific target alloy\n",
    "                        #allocate the mass that is equivalent or less than the demand of target alloy\n",
    "                        temp_alloc=min(temp_IntAlloy_total_mass,ranked_alloy_list[sorted_index]) #make sure the mass in ranked_alloy_list[sorted_index] refers to the same row in \"Int_Target_mass_alloc_by_compnt\" tensor \n",
    "                        Int_Target_mass_alloc_by_compnt[global_row_index_of_k[sorted_index],:,compnt_index]=temp_alloc\n",
    "                        #also record the allocation in terms of AE mass\n",
    "                        Int_Target_AE_alloc_by_compnt[global_row_index_of_k[sorted_index],:,compnt_index]=(temp_alloc/temp_IntAlloy_total_mass_locked)*temp_IntAlloy\n",
    "                        #reduce the allocated mass from total mass of intermediate scrap alloy\n",
    "                        temp_IntAlloy_total_mass=max(0,temp_IntAlloy_total_mass-temp_alloc) \n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "            ##After the line above, (initial) allocation of mass of intermediate scrap alloys of ALL metal groups and ALL components are assigned##\n",
    "    \n",
    "    \n",
    "    \"\"\"For explicitly considering DILUTION (see SI of the manuscript for details of the analytical solution)\"\"\"\n",
    "    if dilution_explicit:     \n",
    "\n",
    "        \"\"\"generate virgin AE added, IntAlloy allocated for offset and virgin alloy still needed after offset use new optimization-based function\"\"\"\n",
    "        output_Int_Target_mass_alloc_by_compnt_updated,output_virgin_AE_mass_by_compnt,output_virgin_alloy_needed_by_compnt,output_virgin_alloy_offset_by_compnt=dilution_AddAE_rebal_combined(Int_Target_AE_alloc_by_compnt,AE_matrix,\n",
    "                                                                                                                                                            TargetAlloy_mass_by_compnt,ingot_alloy_loss_matrix,alpha,num_iter,epsilon,total_utility_score_goal,alloy_group_cutoff)\n",
    "\n",
    "        \n",
    "         \n",
    "        \"\"\"Update outputs (virgin bulk metals, AEs, alloys, leftover mass of intermediate scrap alloys)\"\"\"\n",
    "        ##update leftover intermediate scrap alloy (y,z,j)\n",
    "        for index, cutoff_tuple in enumerate(alloy_group_cutoff):\n",
    "        #if scrap alloy is more than needed for blending, there will be no need for additional virgin alloy, but some leftover scrap or vice versa\n",
    "            #use \"index:index+1\" to avoid the loss of a dimension\n",
    "            IntAlloy_leftover_by_compnt[index,:,:]=IntAlloy_by_compnt[index:index+1,:,:]-np.sum(np.nan_to_num(np.multiply(output_Int_Target_mass_alloc_by_compnt_updated[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:], Int_Target_AE_percent_ini[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:])),axis=0,keepdims=True)\n",
    "        #create a output tensor for IntAlloy_alloc_mass (y,1,j)\n",
    "        IntAlloy_mass_alloc_by_compnt=np.sum(IntAlloy_by_compnt-IntAlloy_leftover_by_compnt,axis=1,keepdims=True)\n",
    "        \n",
    "        ##update the need of virgin AEs (1,z,j)\n",
    "        virgin_AE_mass_by_compnt=np.sum(output_virgin_AE_mass_by_compnt,axis=0,keepdims=True)\n",
    "        \n",
    "        ##update the virgin alloy offset\n",
    "        virgin_alloy_offset_by_compnt=output_virgin_alloy_offset_by_compnt\n",
    "        \n",
    "        ##update the need of virgin alloys (n,1,j)\n",
    "        virgin_alloy_needed_by_compnt-=virgin_alloy_offset_by_compnt #alternatively you can also directly use the output from the new function\n",
    "\n",
    "        ##add the 'AE' (actually bulk+AE) mass that consititutes virgin alloys (accounting for 'ingot-->alloy' loss)\n",
    "        #to be used for calculating energy consumption associated with primary metal and AE demand\n",
    "        #is double-counting of mass (as this AE mass is larger than virgin alloy demand)\n",
    "        temp_AE_comp_tensor=np.expand_dims(AE_matrix,axis=2) #from (n,z) to (n,z,1)\n",
    "        temp_AE_of_virgin_alloy=np.multiply(np.multiply(virgin_alloy_needed_by_compnt,temp_AE_comp_tensor),ingot_alloy_loss_matrix) #(n,z,j)\n",
    "        #update total AE demand\n",
    "        temp_AE_of_virgin_alloy=np.sum(temp_AE_of_virgin_alloy,axis=0,keepdims=True) #calculate total mass of AE by collapsing the along the alloys axis\n",
    "        virgin_AE_mass_by_compnt+=temp_AE_of_virgin_alloy\n",
    "        \n",
    "        ##update Int_Target_mass_alloc_by_compnt\n",
    "        Int_Target_mass_alloc_by_compnt=output_Int_Target_mass_alloc_by_compnt_updated\n",
    "        \n",
    "\n",
    "    else:\n",
    "        \"\"\"Update outputs (virgin alloys,leftover mass of intermediate scrap alloys), if NO DILUTION IS CONSIDERED\"\"\"\n",
    "        ###since no dilution, there will be no demand for virgin bulk metal and AEs (i.e., disregard the quality loss from AE% difference) ###\n",
    "\n",
    "        ##update the leftover intermediate scrap alloy (y,z,j)\n",
    "        for index, cutoff_tuple in enumerate(alloy_group_cutoff):\n",
    "            #update the value of corresponding rows in \"Int_Target_AE_alloc_by_compnt\"\n",
    "            Int_Target_AE_alloc_by_compnt[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:]*=closed_loop_recycl_rate_list[index]\n",
    "            IntAlloy_leftover_by_compnt[index,:,:]=IntAlloy_by_compnt[index:index+1,:,:]-np.sum(Int_Target_AE_alloc_by_compnt[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:], axis=0,keepdims=True)        \n",
    "        #update 'Int_Target_mass_alloc_by_compnt' accordingly\n",
    "        Int_Target_mass_alloc_by_compnt=np.sum(Int_Target_AE_alloc_by_compnt,axis=1,keepdims=True)\n",
    "        #create a output tensor for IntAlloy_alloc_mass (y,1,j)\n",
    "        IntAlloy_mass_alloc_by_compnt=np.sum(IntAlloy_by_compnt-IntAlloy_leftover_by_compnt,axis=1,keepdims=True)\n",
    "        \n",
    "        ##update the virgin alloy offset\n",
    "        virgin_alloy_offset_by_compnt=np.divide(Int_Target_mass_alloc_by_compnt,ingot_alloy_loss_matrix)\n",
    "\n",
    "        ##update the need of virgin alloys (n,1,j)\n",
    "        virgin_alloy_needed_by_compnt-=virgin_alloy_offset_by_compnt\n",
    "        \n",
    "        ##add the 'AE' (actually bulk+AE) mass that consititutes virgin alloys (accounting for 'ingot-->alloy' loss)\n",
    "        #to be used for calculating energy consumption associated with primary metal and AE demand\n",
    "        #is double-counting of mass (as this AE mass is larger than virgin alloy demand)\n",
    "        temp_AE_comp_tensor=np.expand_dims(AE_matrix,axis=2) #from (n,z) to (n,z,1)\n",
    "        temp_AE_of_virgin_alloy=np.multiply(np.multiply(virgin_alloy_needed_by_compnt,temp_AE_comp_tensor),ingot_alloy_loss_matrix) #(n,z,j)\n",
    "        #update total AE demand\n",
    "        temp_AE_of_virgin_alloy=np.sum(temp_AE_of_virgin_alloy,axis=0,keepdims=True) #calculate total mass of AE by collapsing the along the alloys axis\n",
    "        virgin_AE_mass_by_compnt+=temp_AE_of_virgin_alloy  \n",
    "\n",
    "    \n",
    "    return IntAlloy_mass_alloc_by_compnt,virgin_alloy_offset_by_compnt, virgin_alloy_needed_by_compnt,virgin_AE_mass_by_compnt,IntAlloy_leftover_by_compnt,Int_Target_mass_alloc_by_compnt,Int_Target_AE_alloc_by_compnt\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A general function for parsing data for Primary, Manu and EoL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metal_data_parsing(data_sheet_dict):\n",
    "    \"\"\"This module does data parsing for general purpose\n",
    "    ## Input variables:\n",
    "    -data_sheet_dict: a dict contains the data sheets (sheet object) that are used for parsing, in the format of {\"sheet_name\": sheet}\n",
    "    ## Output variables:\n",
    "    -data_parsed_dict: a dict contains the parsed data (in dict), in the format of {\"sheet name\": {metal name processed,process,unit\":[min,max]}}\n",
    "    -data_energy_actions_set_dict: a dict that contains set of actions for metal processing for each data sheet, in the format of {\"sheet name\": (process)}\n",
    "    \"\"\"\n",
    "    #a temp dictionary to store the parsed data for each data sheet\n",
    "    data_parsed_temp=collections.defaultdict(list)\n",
    "    #get a SET of actions\n",
    "    data_energy_actions_set=set()\n",
    "    #an overall dictionary to store all parsed data for all data sheets\n",
    "    data_parsed_dict=collections.defaultdict()\n",
    "    #an overall dictionary to store all unique activities for each data sheet\n",
    "    data_energy_actions_set_dict=collections.defaultdict()\n",
    "    \n",
    "    \"\"\"loop through all the data sheets\"\"\"\n",
    "    for sheet_name, sheet in data_sheet_dict.items():\n",
    "        #read in the cells containing energy data\n",
    "        #select the column first: \n",
    "        data_energy_col=sheet.col_values(2,1) #\"1\" here skips the 0th element (the header in this case)\n",
    "        #select the label column\n",
    "        data_energy_label=sheet.col_values(4,1) \n",
    "        #reset something\n",
    "        data_parsed_temp=collections.defaultdict(list)\n",
    "        data_energy_actions_set=set()\n",
    "        #attach label to the data column: whether a data entry is \"process only\" or \"embedded\"\n",
    "        for i in range (len(data_energy_col)):\n",
    "            if data_energy_label[i]==0:\n",
    "                data_energy_col[i]+=','+\"Proc\"\n",
    "            else:\n",
    "                data_energy_col[i]+=','+\"Embed\"\n",
    "        #split the string of each cell\n",
    "        for cell in data_energy_col:\n",
    "            temp=[str(i) for i in cell.split(\",\")] #change each split element from unicode to str\n",
    "            temp[2]=float(temp[2]) #change the actual data to float type\n",
    "            data_energy_actions_set.add(temp[1])\n",
    "        #reformat and append the information to each key\n",
    "            data_parsed_temp[str(temp[0]).upper()+\",\"+str(temp[1]).upper()+\",\"+str(temp[-1]).upper()].append(temp[2])\n",
    "        #get min and max value for each key in the dict\n",
    "        for key, value in data_parsed_temp.items():\n",
    "            max_=reduce(lambda a,b: a if a>b else b,value) #you need 'reduce' to do the iteration, althought it does not go into nested lists\n",
    "            min_=reduce(lambda a,b: a if a<b else b,value)\n",
    "            data_parsed_temp[key]=[min_,max_]\n",
    "        #assign parsed data to corresponding dicts\n",
    "        data_parsed_dict[sheet_name]=data_parsed_temp\n",
    "        data_energy_actions_set_dict[sheet_name]=data_energy_actions_set\n",
    "    return data_parsed_dict,data_energy_actions_set_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unique_data_expansion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_data_expansion(EO_basic,EO_extended_blank,mapping_dict):\n",
    "    \"\"\"This function copys the process energy data of unique category (e.g., 'Al hot rolled wrought') and paste it to all the relevant subcategories (e.g., 'Al 1070A hot rolled wrought')\n",
    "    ## Input variables\n",
    "    -EO_basic: a dataframe that contains the energy data for basic (unique) processes; dimension=(1,# of unique processes) or (# of combinations due to variation in energy data,# of unique processes)\n",
    "    -EO_extended_blank: a dataframe that contains the zeros for all processes in y or PIOT; dimension=(1, total # of processes) or (# of combinations due to variation in energy data,total # of processes)\n",
    "    -mapping_dict: contains {index_basic_EO: list[index1_extend_EO, index2_extend_EO...)}\n",
    "    ## Output variables\n",
    "    -EO_extended: a dataframe that contains the energy data for all processes in y or PIOT; dimension=(1, total # of processes) or (# of combinations due to variation in energy data,total # of processes)\n",
    "    \"\"\"\n",
    "    \n",
    "    ##loop through the mapping dict\n",
    "    for col_basic, list_col_index_ext in mapping_dict.items():\n",
    "        for col_index_ext in list_col_index_ext:\n",
    "            EO_extended_blank.iloc[:,col_index_ext]=EO_basic.iloc[:,col_basic].values[0] #add '.value[0]' because now you only have one row of EO\n",
    "    ##make a copy to output variable\n",
    "    EO_extended=EO_extended_blank\n",
    "    \n",
    "    return EO_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data parsing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_parsing_MOD(metal_process_data_workbook,body_composition_workbook,num_new_vehicles_dict,num_retired_vehicles_dict,time_span_tuple,compnt_loc_tuple_list,max_or_min):\n",
    "    \"\"\"This module does data parsing for the following output variables:\n",
    "    ##Input variables\n",
    "    -metal_process_data_workbook: contains worksheets of \"Primary\", \"Manu\" and \"EoL treatment\" that contain process data\n",
    "    -body_composition_workbook: contains worksheet of the vehicle body composition data\n",
    "    -num_new_vehicles_dict: a dict contains the new vehicle demand, in the format of {\"vehicle type\":[{year1:amount1},{year2:amount2}..]}\n",
    "    -num_retired_vehicles_dict: a dict contains the # of vehicle retired, in the format of {\"vehicle type\":[{year1:amount1},{year2:amount2}..]}\n",
    "    -time_span_tuple: a tuple contains the starting and ending years of the time span for modeling, identical to what's specified in MFA module    \n",
    "    -compnt_loc_tuple_list: store the start and end rows (both inclusive) of a compnt in material composition tab \"Alloy_compnt_G&L\"\n",
    "    -max_or_min: a boolean variable to decide whether use the max or min values of EO for subsequent calculations\n",
    "    ## Output variables\n",
    "    -PIOT_A_sheet: a dataframe of intersectoral coefficients, nxn matrix\n",
    "    -ES_sheet: a dataframe of energy source share (in fraction) for each sector, mxn matrix\n",
    "    -eff_sheet: energy use efficiency for each energy source, mx1 vector\n",
    "    -EF_sheet: emission factors for each energy source, 1xm row vector\n",
    "    -energy_data_parsed_overall_dict: a dictonary of parsed process energy data for stages of interest,in the format of {\"stage name\": {\"metal name processed,process,unit\":[min,max]}}\n",
    "    -energy_activity_parsed_overall_dict: a dictonary of parsed process name for stages of interest,in the format of {\"stage name\": (process)}\n",
    "    -EO_sheet_final_extended: a dataframe with col_name being materials sectors and row being process energy data (max or min) for each sector\n",
    "                                in the shape of (1, n)   \n",
    "    -data_body_composition_parsed: material composition of vehicles by type, in the format of {'vehi_type':[{material:mass}]}\n",
    "    -y_total_alloy_time_series_tensor: a 4D tensor that contains total alloys demand (n) by component (j) for each vehicle type (channel 1=c), for each year (channel 2=z), kg\n",
    "    -y_total_scrap_potential_time_series_tensor: a 4D tensor that contains total potential scrap alloys available (n) by component (j) for each vehicle type (channel 1=c), for each year (channel 2=t), kg\n",
    "    -alloy_AE_spec_matrix: a matrix of alloys (row) and their AE specs (col), in %, (n,z)\n",
    "    -mat_compnt_tensor: materials (row) by components (col) matrix that shows the material composition of each component, \n",
    "       in normalized mass (kg/kg), (n,j), for all vehicle types (c), (n,j,c)\n",
    "    -ingot_alloy_loss_matrix: a matrix of alloys from secondary (row) and corresponding cumulative loss from 'ingot' stage to 'alloy' stage, (n,1)\n",
    "    -TD_matrix: thermodynmic limit matrix that contains the yield of AE (col) during remelting of scrap allys of same group (row),\n",
    "        in %, (y,z)\n",
    "    -total_new_vehi_time_series_tensor: total number of vehicles (row=1) by vehicle type (col=c) by year, (1,c,t)\n",
    "    \n",
    "    ###Caution:\n",
    "    *There is a \"compnt_loc_tuple_list\" variable that is hard-coded to store the start and end rows (both inclusive) of a compnt\n",
    "    *check for \"====hard coding warning====\"\n",
    "    *mixed use of 'z' and 't' for time dimension \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"Load data files\"\"\"\n",
    "    ##load in workbooks named \"data_input_lightweight\",\"data_input_battery\",\"data_input_composition\"\n",
    "    data_metal_process_loaded=xlrd.open_workbook(metal_process_data_workbook)\n",
    "    body_composition_loaded=xlrd.open_workbook(body_composition_workbook)\n",
    "    #for material manufacturing and EoL\n",
    "    primary_part_sheet=data_metal_process_loaded.sheet_by_name(\"Primary_part\") #get to the actual tab\n",
    "    manu_part_sheet=data_metal_process_loaded.sheet_by_name(\"Manu_part\")\n",
    "    EoL_part_sheet=data_metal_process_loaded.sheet_by_name(\"EoL_part\")\n",
    "    #for composition\n",
    "    body_composition_sheet=body_composition_loaded.sheet_by_name(\"Alloy_compnt_G&L\")\n",
    "    body_composition_percent_sheet=body_composition_loaded.sheet_by_name(\"Alloy_compnt_G&L_percent\")\n",
    "    #body_composition_sheet_virgin=body_composition_loaded.sheet_by_name(\"Archetype_body_GREET2_(virgin)\") #no scrap reuse\n",
    "    #for AE spec of alloys\n",
    "    AE_spec_sheet=body_composition_loaded.sheet_by_name(\"Alloy_spec\")\n",
    "\n",
    "    \"\"\"Parse PIOT_A, ES, eff, EF\"\"\"\n",
    "    #use pandas here, in the future, may need to reconcile with xlrd\n",
    "    xls_file=pd.ExcelFile(metal_process_data_workbook)\n",
    "    #get PIOT_A\n",
    "    PIOT_A_sheet=pd.read_excel(xls_file,\"PIOT_A\",usecols=lambda x: 'Unnamed' not in x) #remove the first col\n",
    "    print (\"dimension of PIOT_A is: \",PIOT_A_sheet.shape,\"\\n\")\n",
    "    #get ES\n",
    "    ES_sheet=pd.read_excel(xls_file,\"PIOT_ES\",usecols=lambda x: 'Unnamed' not in x) #remove the first col\n",
    "    print (\"dimension of ES is: \",ES_sheet.shape,\"\\n\")\n",
    "    #get eff\n",
    "    eff_sheet=pd.read_excel(xls_file,\"PIOT_eff\",usecols=lambda x: 'Unnamed' not in x) #remove the first col\n",
    "    eff_sheet.drop([\"ref\",\"ref2\",\"Energy use efficiency\",\"Life cycle energy\"],inplace=True,axis=1)\n",
    "    print (\"dimension of eff is: \",eff_sheet.shape,\"\\n\")\n",
    "    #get EF\n",
    "    EF_sheet=pd.read_excel(xls_file,\"PIOT_EF\",usecols=lambda x: 'Unnamed' not in x) #remove the first col\n",
    "    EF_sheet.drop(\"ref\",inplace=True,axis=1) #if inplace=False (default), the original EF_sheet will not be changed, only copy will be altered\n",
    "    EF_sheet=EF_sheet.T\n",
    "    print (\"dimension of EF is: \",EF_sheet.shape,\"\\n\")\n",
    "    \n",
    "    #for 'ingot' to 'alloy' loss factors\n",
    "    ingot_alloy_loss_matrix=pd.read_excel(xls_file,\"Ingot_alloy_loss_matrix\",usecols=lambda x: 'Unnamed' not in x) #remove the first col\n",
    "    print (\"dimension of ingot-alloy-loss matrix is: \",ingot_alloy_loss_matrix.shape,\"\\n\")\n",
    "    #print (\"values of ingot-alloy-loss matrix: \", ingot_alloy_loss_matrix,\"\\n\")\n",
    "\n",
    "    #for 'TD_filter'\n",
    "    TD_matrix=pd.read_excel(xls_file,\"TD_filter\",usecols=lambda x: 'Unnamed' not in x) #remove the first col\n",
    "    #print (\"TD_matrix first import: (shpae)\", TD_matrix.shape,\"\\n\")\n",
    "    #print (\"TD_matrix first import: (headers)\", TD_matrix.columns,\"\\n\")\n",
    "           \n",
    "    \"\"\"Parse vehicle composition data\"\"\"\n",
    "    #a dictionary to store the reformated data\n",
    "    data_body_composition_parsed=collections.defaultdict()\n",
    "    #read in the cells containing body composition data\n",
    "    #get a list of vehicle types, components\n",
    "    temp_vehicle_types=[]\n",
    "    temp_vehicle_compnts=set() \n",
    "    \n",
    "    #====hard coding warning====#\n",
    "    num_vehi_cols=body_composition_sheet.ncols-15 #15 is number of columns that are (necessary but) not vehicle\n",
    "    \n",
    "    #new for material composition by components: a list to store the start and end rows (both inclusive) of a compnt\n",
    "    #now an input variable\n",
    "    #compnt_loc_tuple_list=[(2,61), (63,122),(124,183),(185,244),(246,305),(307,366),(368,427)]\n",
    "\n",
    "    #prepare dimension for alloys used a vehicle, compnts and vehi types; for number of rows for alloys, need to do a calculation\n",
    "    dim_nc,dim_j,dim_c=int((body_composition_sheet.nrows-1)/len(compnt_loc_tuple_list)-1),int(len(compnt_loc_tuple_list)),int(num_vehi_cols)\n",
    "    \"\"\"caution!!! total metals (rows=ny) for y tensor (defined later in the module) is larger than total alloys (rows=nc) for vehicle components!!!!\"\"\"\n",
    "    \n",
    "    #create a vehicle composition tensor (nc,j,c)\n",
    "    alloy_compnt_vehi_tensor=np.zeros((dim_nc,dim_j,dim_c))\n",
    "    #print alloy_compnt_vehi_tensor.shape\n",
    "\n",
    "    ##create a dict of entire composition table of all vehi types  \n",
    "    #format:{'vehi_type':{compnt:[{'steel hot rolled stamped': xxx kg}, {'steel cold rolled stamped': yyy kg}...] } }\n",
    "    for i in range(num_vehi_cols): \n",
    "        temp_vehicle_types.append(str(body_composition_sheet.cell_value(0,i+1))) \n",
    "        #for each index (vehi type), further create a dict of lists to store material data\n",
    "        data_body_composition_parsed[temp_vehicle_types[i]]=collections.defaultdict(list)\n",
    "        for compnt_index, compnt_tuple in enumerate (compnt_loc_tuple_list):\n",
    "            #create a index entry with corresponding compnt name, and keep assigning values of materials to its list \n",
    "            temp_vehicle_compnts.add(str(body_composition_sheet.cell_value(compnt_tuple[0]-1,0)))\n",
    "            for alloy_index, j in enumerate (range(compnt_tuple[0],compnt_tuple[1]+1)):\n",
    "                #data_body_composition_parsed[temp_vehicle_types[i-1]].append({str(body_composition_sheet.cell_value(j,0)):float(body_composition_sheet.cell_value(j,i))})    \n",
    "                data_body_composition_parsed[temp_vehicle_types[i]][str(body_composition_sheet.cell_value(compnt_tuple[0]-1,0))].append({str(body_composition_sheet.cell_value(j,0)):float(body_composition_sheet.cell_value(j,i+1))})\n",
    "                #popluate the \"alloy composition by compnt by vehi type\" tensor\n",
    "                alloy_compnt_vehi_tensor[alloy_index,compnt_index,i]=float(body_composition_sheet.cell_value(j,i+1))\n",
    "                    \n",
    "    #create the alloy_AE_sepc_matrix (#alloys,#AE)\n",
    "    dim_AE=AE_spec_sheet.ncols-1\n",
    "    alloy_AE_spec_matrix=np.zeros((dim_nc,dim_AE))\n",
    "    for i in range (dim_nc):\n",
    "        for j in range (dim_AE):\n",
    "            alloy_AE_spec_matrix[i,j]=float(AE_spec_sheet.cell_value(i+1,j+1)) #0-index, row=0 is the header\n",
    "\n",
    "    #create mat_compnt_tensor (kg alloy/kg component,#component,#vehi_type)\n",
    "    mat_compnt_tensor=np.zeros((dim_nc,dim_j,dim_c))\n",
    "    for l in range (dim_c):\n",
    "        for j in range (dim_j):\n",
    "            for i in range (dim_nc):\n",
    "                #first two rows are: overal header (vehicle types) and subheader of 1st compont\n",
    "                mat_compnt_tensor[i,j,l]=float(body_composition_percent_sheet.cell_value(i+2+j*(dim_nc+1),l+1))\n",
    "    \n",
    "    #print (\"percentage of Steel galvanized stamped in power system in lightweight ICEV: \",mat_compnt_tensor[2,1,1],\"\\n\")\n",
    "    \n",
    "    \"\"\"Parse process energy data\"\"\"\n",
    "    #prepare the data sheet dict for the function\n",
    "    data_sheet_input_dict={'Primary_part':primary_part_sheet,'Manu_part':manu_part_sheet,'EoL_part':EoL_part_sheet}\n",
    "    energy_data_parsed_overall_dict,energy_activity_parsed_overall_dict=metal_data_parsing(data_sheet_input_dict)\n",
    "\n",
    "       \n",
    "    \"\"\"create dataframes to for creating EO\"\"\"\n",
    "    ##create dataframe for each stage from dict, using min values\n",
    "    temp_dict_min=collections.defaultdict(float)\n",
    "    for sheet_name in data_sheet_input_dict.keys():\n",
    "        for key,value in energy_data_parsed_overall_dict[sheet_name].items():\n",
    "            if value[0] < temp_dict_min[key.split(\",\")[0]] or not temp_dict_min[key.split(\",\")[0]]:\n",
    "                temp_dict_min[key.split(\",\")[0]]=value[0] \n",
    "\n",
    "    #CAUTION: the order of the data entries are based on hash value, NOT the order in your PIOT.\n",
    "    df_process_energy_min_concat=pd.DataFrame.from_dict(temp_dict_min,orient=\"index\") \n",
    "    df_process_energy_min_concat.columns=['Process energy (MJ/kg)']          \n",
    "                \n",
    "    ##create dataframe for each stage from dict, using max values\n",
    "    temp_dict_max=collections.defaultdict(float)\n",
    "    for sheet_name in data_sheet_input_dict.keys():\n",
    "        for key,value in energy_data_parsed_overall_dict[sheet_name].items():\n",
    "            if value[1] > temp_dict_max[key.split(\",\")[0]] or not temp_dict_max[key.split(\",\")[0]]:\n",
    "                temp_dict_max[key.split(\",\")[0]]=value[1] \n",
    "\n",
    "    #CAUTION: the order of the data entries are based on hash value, NOT the order in your PIOT.\n",
    "    df_process_energy_max_concat=pd.DataFrame.from_dict(temp_dict_max,orient=\"index\") \n",
    "    df_process_energy_max_concat.columns=['Process energy (MJ/kg)']\n",
    "   \n",
    "\n",
    "    \"\"\"Parse demand data (for y tensor) from MFA module\"\"\"\n",
    "    #get a copy of the sheet with all materials (not just alloys in vehicles, but also intermediates such as \"steel hot rolled\")\n",
    "    y_sheet=pd.read_excel(xls_file,\"PIOT_y_by_compnt\",usecols=lambda x: 'Unnamed' not in x) #remove the first col\n",
    "    #reset all values to zero\n",
    "    y_sheet[:]=0.0  \n",
    "    #create a dim for rows of y sheet: number of rows are more than what's in vehicle composition sheet, \n",
    "    #  as you have intermediate materials (e.g. aluminum ingot) that are not present in vehicle composition\n",
    "    #also create a dim for time span\n",
    "    dim_ny, dim_z=y_sheet.shape[0], time_span_tuple[1]-time_span_tuple[0]+1\n",
    "    #record the total time span (current time span could be a fraction of\"total time span\" from the MFA)\n",
    "    #e.g., total tim span can be 1999 to 2050, while current time span could be 2017 to 2030\n",
    "    #so, just pick one vehi type and get start and end years of total time span\n",
    "    total_time_span_tuple=(list(num_new_vehicles_dict['PHEV_conventional'][0].keys())[0],list(num_new_vehicles_dict['PHEV_conventional'][-1].keys())[0])\n",
    "    #then create a locator to locate the index corresponds to the year in the list of {year:vehi_vol}s\n",
    "    year_index_locator=time_span_tuple[0]-total_time_span_tuple[0]\n",
    "    \n",
    "    ##populate the y tensors (ny,j,c,z) for total alloy demand and potential scrap availability (other rows remain zero)\n",
    "    #create an empty tensor\n",
    "    y_total_alloy_time_series_tensor=np.zeros((dim_ny,dim_j,dim_c,dim_z))\n",
    "    y_total_scrap_potential_time_series_tensor=np.zeros((dim_ny,dim_j,dim_c,dim_z))\n",
    "    total_new_vehi_time_series_tensor=np.zeros((1,dim_c,dim_z)) #total volume of new vehicle demand by vehicle type by year (sorry for the confusion, should use t for time)\n",
    "       \n",
    "    for vehi_index, vehi_type in enumerate (temp_vehicle_types):\n",
    "        for year_index, year in enumerate (range(time_span_tuple[0],time_span_tuple[1]+1)):\n",
    "            for compnt_index, compnt_type in enumerate (temp_vehicle_compnts):\n",
    "                for alloy_index in range(alloy_compnt_vehi_tensor.shape[0]): #the first nc rows that corrspond to virgin alloys\n",
    "                    #calculate material (total alloy only) demand from \"num_new_vehicles_dict\" \n",
    "                    y_total_alloy_time_series_tensor[alloy_index,compnt_index,vehi_index,year_index]=alloy_compnt_vehi_tensor[alloy_index,compnt_index,vehi_index]*num_new_vehicles_dict[vehi_type][year_index+year_index_locator][year]\n",
    "                    #calculate the potential available scrap from \"num_retired_vehicles_dict\"; first row of scrap alloys starting after the last row of virgin alloys (i.e., nc+1)\n",
    "                    y_total_scrap_potential_time_series_tensor[alloy_index+dim_nc,compnt_index,vehi_index,year_index]=alloy_compnt_vehi_tensor[alloy_index,compnt_index,vehi_index]*num_retired_vehicles_dict[vehi_type][year_index+year_index_locator][year]\n",
    "            #re-organize annual new vehicle demand in to (1,c,z=t)\n",
    "            total_new_vehi_time_series_tensor[:,vehi_index,year_index]=num_new_vehicles_dict[vehi_type][year_index+year_index_locator][year]\n",
    "    #One more step for scrap: merge the potential Mg scraps to one entry\n",
    "    y_total_scrap_potential_time_series_tensor[dim_nc*2-2,:,:,:]+=y_total_scrap_potential_time_series_tensor[dim_nc*2-1,:,:,:]\n",
    "    y_total_scrap_potential_time_series_tensor[dim_nc*2-1,:,:,:]=0\n",
    "        \n",
    "    \n",
    "    \"\"\"generate two versions of EO here (all min, all max)\"\"\"\n",
    "    ##Prepare variables for EO\n",
    "    EO_sheet=pd.read_excel(xls_file,\"PIOT_EO_basic\",usecols=lambda x: 'Unnamed' not in x) #remove the first col, start with basic version\n",
    "    EO_sheet[:]=0.0 #change NaNs to zeros\n",
    "    \n",
    "    #get a list of EO_sheet column headers in string\n",
    "    EO_sheet_col_name_list=[str(col_name).upper() for col_name in EO_sheet.columns]\n",
    "    #transpose the dataframe of process energy from Data_parsing_MOD\n",
    "    df_process_energy_min_concat_trans=df_process_energy_min_concat.T\n",
    "    df_process_energy_max_concat_trans=df_process_energy_max_concat.T\n",
    "\n",
    "    ##filter out the data entries from 'df_process_energy_trans' that are not used in EO\n",
    "    #create a col name list for 'df_process_energy_trans'\n",
    "    df_process_energy_col_name_list=[str(col_name).upper() for col_name in df_process_energy_max_concat_trans.columns]\n",
    "    #create a list of col name that does not exist in EO sheet\n",
    "    no_use_col_name_list=[col_name for col_name in df_process_energy_col_name_list if not (col_name in EO_sheet_col_name_list)]\n",
    "    #create a list of col name that are in EO sheet but missing from 'df_process_energy'\n",
    "    missing_col_name_list=[col_name for col_name in EO_sheet_col_name_list if not (col_name in df_process_energy_col_name_list)]\n",
    "    #now, drop the columns in df_process energy_trans that are not used for EO\n",
    "    df_process_energy_min_concat_trans.drop(no_use_col_name_list,axis=1, inplace=True)\n",
    "    df_process_energy_max_concat_trans.drop(no_use_col_name_list,axis=1, inplace=True)\n",
    "    \n",
    "    ##export 'df_process_energy_min_concat_trans' and 'df_process_energy_max_concat_trans' to excel\n",
    "    df_process_energy_max_concat_trans.to_excel(\"check_max_proc_energy_47_col.xlsx\")\n",
    "    df_process_energy_min_concat_trans.to_excel(\"check_min_proc_energy_47_col.xlsx\")\n",
    "    ##[caution] the order of columns is different between 'df_process_energy_xxx_concat_trans' and 'EO_sheet_min/max' \n",
    "       \n",
    "    EO_sheet_min=EO_sheet.copy()\n",
    "    EO_sheet_min.rename(columns=str.upper,inplace=True) #need to change column name to upper case\n",
    "    EO_sheet_max=EO_sheet.copy()\n",
    "    EO_sheet_max.rename(columns=str.upper,inplace=True) #need to change column name to upper case\n",
    "    for col in EO_sheet_min.columns:\n",
    "        EO_sheet_min.loc[:,col]=df_process_energy_min_concat_trans.loc[:,col][0]\n",
    "        EO_sheet_max.loc[:,col]=df_process_energy_max_concat_trans.loc[:,col][0]\n",
    "        \n",
    "    print (\"dimension of EO_basic is: \", EO_sheet.shape,\"\\n\")\n",
    "    ## export 'EO_sheet_min' and 'EO_sheet_max' to excel\n",
    "    EO_sheet_min.to_excel(\"EO_sheet_min.xlsx\")\n",
    "    EO_sheet_max.to_excel(\"EO_sheet_max.xlsx\")\n",
    "    \n",
    "    if max_or_min=='max':\n",
    "        EO_sheet_final=EO_sheet_max\n",
    "    else:\n",
    "        EO_sheet_final=EO_sheet_min\n",
    "        \n",
    "    \n",
    "    \"\"\"expand from basic version of PIOT_EO to full length version of PIOT_EO\"\"\"\n",
    "    ##The EO compiled above contains the entry like \"Al hot rolled wrought\", now we need to expand this to relevant alloys\n",
    "    ##It's essentially a problem of multiplying the value of a col for a given times\n",
    "    \n",
    "    ##create input variables to the function\n",
    "    EO_extended_blank=pd.read_excel(xls_file,\"PIOT_EO\",usecols=lambda x: 'Unnamed' not in x) #remove the first col, create a full version with one row of zeros\n",
    "    EO_extended_blank=pd.DataFrame(data=np.zeros((EO_sheet_final.shape[0],EO_extended_blank.shape[1]))) #expand rows to total number of combos\n",
    "    temp_mapping_sheet=pd.read_excel(xls_file,\"EO_basic_to_EO_all\",usecols=lambda x: 'Unnamed' not in x) #remove the first col, create a copy of mapping tab\n",
    "    #create the mapping dict\n",
    "    mapping_dict=collections.defaultdict(list)\n",
    "    #zip the \"EO_basic_col_index\" and \"EO_full_col_index\" columns\n",
    "    zipped_col_index=zip(temp_mapping_sheet.loc[:,\"Col_index in EO_basic\"],temp_mapping_sheet.loc[:,\"col_index in EO_all entries\"])\n",
    "    for col_index_tuple in zipped_col_index:\n",
    "        mapping_dict[col_index_tuple[0]].append(col_index_tuple[1])\n",
    "\n",
    "    EO_sheet_final_extended=unique_data_expansion(EO_sheet_final,EO_extended_blank, mapping_dict)\n",
    "    print (\"dimension of EO (total ver): \", EO_sheet_final_extended.shape,\"\\n\")\n",
    "    ## export EO total ver for checking\n",
    "    EO_sheet_final_extended.to_excel(\"EO_extended_ver.xlsx\")\n",
    "\n",
    "   \n",
    "    return total_new_vehi_time_series_tensor,TD_matrix,ingot_alloy_loss_matrix,dim_ny, mat_compnt_tensor, alloy_AE_spec_matrix, data_body_composition_parsed,EO_sheet_final_extended,y_total_alloy_time_series_tensor,y_total_scrap_potential_time_series_tensor,PIOT_A_sheet,ES_sheet,eff_sheet,EF_sheet\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EoL module that explicitly deal with dismantling, shredding, sorting and blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EoL_MOD(v_c_tensor,v_s_tensor,mat_compnt_tensor,AE_matrix,shredding_loss_rate,TD_matrix,alloy_group_cutoff,\n",
    "            TargetAlloy_mass_by_compnt_by_vehi,ingot_alloy_loss_matrix,\n",
    "            alpha,num_iter,epsilon,total_utility_score_goal,closed_loop_recycl_rate_list,dilution_explicit,\n",
    "            rand_sort_error=0.02,cross_mixing_flag=True):\n",
    "    \"\"\"This module calculates the scrap generation from vehicle dismantling, shredding, sorting and blending. \n",
    "    Also calculated is the reuse amount of scrap and corresponding need for virgin bulk metals, AEs and virgin alloys \n",
    "    (that are still needed to fulfill the demand). \n",
    "    THE CALCULATION IS FOR ONE YEAR\n",
    "    \n",
    "    ## Input variables:\n",
    "    ***Input variables below are for Dismantling***\n",
    "    -v_c_tensor: a 3D tensor contains vectors containing the components (j) of a specific type of vehicle (1) for c types, in total mass (kg), (j,1,c)\n",
    "    -v_s_tensor: a 3D tensor contains vectors containing the dismantling rate of each component (rest is left to \"car hulk\"), corresponding to v_c_tensor, in %, (j,1,c)\n",
    "        note that \"reuse/remanufacturing\" (e.g., remanu of engine) is not considered; \"dismantling rate\" strictly refers to the portion that is \"unwanted and shredded separately (from other components or hulk)\"\n",
    "    -mat_compnt_tensor: materials (row) by components (col) matrix that shows the material composition of each component, \n",
    "       in normalized mass (kg/kg), (n,j), for all vehicle types (c), (n,j,c)\n",
    "    ***Input variables below are for Shredding***\n",
    "    -AE_matrix: alloys (row) by alloying elements (col) matrix that shows the AE composition of each alloy, in %, (n,z)\n",
    "    -shredding_loss_rate: a list contains the shredding loss rate for each component (len=j) or car hulk (len=1)\n",
    "    ***Input variables below are for Sorting&Remelting***\n",
    "    -TD_matrix: thermodynmic limit matrix that contains the yield of AE (col) during remelting of scrap allys of same group (row),\n",
    "        in %, (y,z)\n",
    "    -rand_sort_error: a random sorting error rate (scalar) that indicates the imperfect sorting \n",
    "    -alloy_group_cutoff: a list contains tuple of (staring,ending) rows in the alloy-related matrix that belongs to a certain common metal group,\n",
    "        len()= #of metal types = y\n",
    "    ***Input variables below are for Blending***\n",
    "    -TargetAlloy_mass_by_compnt_by_vehi:a 4D tensor that contains target alloy by metal (row), by AE composition (col), \n",
    "        for each component OR car hulk (channel) and each vehicle type (channl2) in total mass (kg), (n,z,j,c)\n",
    "    -ingot_alloy_loss_matrix: a matrix of alloys from secondary (row) and corresponding cumulative loss from 'ingot' stage to 'alloy' stage, (n,1)\n",
    "    -dilution_explicit: a boolean variable to indicate whether \"dilution (quality loss)\" is explicitly modeled\n",
    "    -closed_loop_recycl_rate_list: a list containing the metal group-lvl closed-loop recycling rate factors, in %\n",
    "    -alpha: learning rate, scalar (dim=0)\n",
    "    -num_iter: number of iterations before the gradient descent algorithm is terminated, scalar (dim=0)\n",
    "    -epsilon: a small value to create margins of AE_spec%, in %\n",
    "    -total_utility_score_goal: a small value to compare with total utility score of an iteration as the termination criteron , scalar (dim=0)\n",
    "\n",
    "\n",
    "    ##Output variables: (OUTDATED)\n",
    "    -virgin_bulk_metal_mass_by_compnt_output: a 3D tensor containing virgin bulk metals (row), by total mass (col), for each component OR car hulk (channel), in total mass (kg), (y,1,j)\n",
    "    -virgin_AE_mass_by_compnt_output: a 3D tensor containing (1 row of) virgin AEs needed (col), for each component OR car hulk (channel), in total mass (kg), (1,z,j)\n",
    "    -virgin_alloy_needed_by_compnt_output: a 3D tensor that stores the mass (col=1) of each target alloy from virgin sources(row),\n",
    "        for each component OR car hulk (channel), in total mass (n,1,j)\n",
    "    -Int_Target_mass_alloc_by_compnt_output: a 3D tensor that stores the mass of intermediate scrap alloy (col=1) allocated to each target alloy (row),\n",
    "        for each component OR car hulk (channel), in total mass (n,1,j)\n",
    "    -Int_Target_AE_alloc_by_compnt_output: a 3D tensor that stores the mass of AEs (including bulk) that constitute intermediate scrap alloy (col=z),\n",
    "        allocated to each target alloy (row),for each component OR car hulk (channel), in total mass (n,z,j)\n",
    "    -IntAlloy_leftover_by_compnt_output: a 3D tensor that stores the mass (col=1) of each intermediate scrap alloy that is not used up (row),\n",
    "        for each component OR car hulk (channel), in total mass (y,1,j)\n",
    "  \n",
    "    ###KEYWORDS for NAMING CONVENTION\n",
    "    \"hulk\": label variables where handling scraps as a whole hulk\n",
    "    \"SepCom\": lable variables where handling scraps separately by individual component \n",
    "    \"cross\": lable variables where \"cross-mixing\" is assumed\n",
    "    \"no_cross\": lable variables where \"no cross-mixing\" is assumed\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"Common steps (Dismantling)\"\"\"\n",
    "    ##Dismantling: this generates 3D tensors that contain mass of scraps by component for each (single) vehicle type\n",
    "    #e.g., xx kg scrap \"AL 1070A hot rolled wrought\" from \"transmission\" of \"BEV\"\n",
    "    scrap_compnt_to_SepCom_tensor,scrap_compnt_to_hulk_tensor=Dismantling(v_c_tensor,v_s_tensor,mat_compnt_tensor)\n",
    "        \n",
    "    \n",
    "    \"\"\"Arrange the following steps based on 'cross-mixing' or 'no cross-mixing' \"\"\"\n",
    "    if cross_mixing_flag:\n",
    "        ##====Shredding====##\n",
    "        #for dismantled compnts (the tensors are 3D (alloy,AE,compnt),the 4th dimension \"vehi_type\" has been collpased)\n",
    "        shredded_alloys_mix_by_compnt_SepCom_cross,shred_loss_to_others_SepCom_cross,to_be_shredded_copy_SepCom_cross=Shredding(scrap_compnt_to_SepCom_tensor,AE_matrix,shredding_loss_rate,cross_mixing=cross_mixing_flag)\n",
    "        #for hulk\n",
    "        shredded_alloys_mix_by_compnt_hulk_cross,shred_loss_to_others_hulk_cross,to_be_shredded_copy_hulk_cross=Shredding(scrap_compnt_to_hulk_tensor,AE_matrix,shredding_loss_rate,cross_mixing=cross_mixing_flag)\n",
    "\n",
    "        ##====Sorting&remelting====##\n",
    "        \"\"\"Only did for shreds from separated compnts, assume shred from hulk won't be used for closed-loop recycling\"\"\"\n",
    "        IntAlloy_by_compnt_cross,remelt_loss_to_others_cross,to_be_remelted_copy_cross=Sorting_and_remelting(shredded_alloys_mix_by_compnt_SepCom_cross,alloy_group_cutoff,\n",
    "                                                                                                             TD_matrix,rand_sort_error=rand_sort_error)\n",
    "\n",
    "        \n",
    "        \"\"\"create the 'contribute_ratio' tensor to create the 4D tensor version of IntAlloy_by_compnt_cross\"\"\"\n",
    "        #the IntAlloy_by_compnt_cross from Sorting&remelting step is 3D (y,z,j), because the dimension of vehicle types has been collapsed\n",
    "        \n",
    "        #initiate \"contribute_ratio_SepCom_cross\"\n",
    "        contribute_ratio_SepCom_cross=np.ones((IntAlloy_by_compnt_cross.shape[0],IntAlloy_by_compnt_cross.shape[2],scrap_compnt_to_SepCom_tensor.shape[2])) #(y,j,c)\n",
    "        for index,cutoff_tuple in enumerate(alloy_group_cutoff):\n",
    "            temp_sum_shredded=np.sum(to_be_shredded_copy_SepCom_cross[cutoff_tuple[0]:cutoff_tuple[1]+1,:,:],axis=0,keepdims=True) #collapse k rows to one row, while keep the other two dimeions unchanged; (1,j,c)\n",
    "            contribute_ratio_SepCom_cross[index,:,:]=np.divide(temp_sum_shredded,np.sum(temp_sum_shredded,axis=2,keepdims=True))\n",
    "        #change 'nan' to zero; otherwise when you have zero mass of a metal group (e.g., Mg), you will have a 'nan' contribute ratio\n",
    "        #which, after being multiplied with element in 'IntAlloy_by_compnt_cross tensor', will lead to 'nan' that cannot be plotted\n",
    "        contribute_ratio_SepCom_cross=np.nan_to_num(contribute_ratio_SepCom_cross)\n",
    "        \n",
    "        #create the 4D tensor version of IntAlloy_by_compnt_cross (y,z,j,c)\n",
    "        IntAlloy_by_compnt_cross=np.multiply(np.expand_dims(IntAlloy_by_compnt_cross,axis=3),np.expand_dims(contribute_ratio_SepCom_cross,axis=1))\n",
    "\n",
    "\n",
    "        ##====Blending====##\n",
    "        \"\"\"similar to Sorting&remelting, only separated compnts are investigated here\"\"\"\n",
    "        \n",
    "        \"\"\"create corresponding 4d tensors\"\"\"\n",
    "        ##create 4D tensors to hold all the blending results of scraps of all vehicle types\n",
    "        #tensors with alloys as rows, AE/total mass as col, compont as channel1, vehi_type as channel2\n",
    "        blend_dim_n,blend_dim_z,blend_dim_j,blend_dim_c=(mat_compnt_tensor.shape[0],AE_matrix.shape[1],\n",
    "                                                        mat_compnt_tensor.shape[1],mat_compnt_tensor.shape[2])\n",
    "        Int_Target_mass_alloc_by_compnt_cross=np.zeros((blend_dim_n,1,blend_dim_j,blend_dim_c))\n",
    "        virgin_alloy_needed_by_compnt_cross=np.zeros((blend_dim_n,1,blend_dim_j,blend_dim_c))\n",
    "        virgin_alloy_offset_by_compnt_cross=np.zeros((blend_dim_n,1,blend_dim_j,blend_dim_c))\n",
    "        Int_Target_AE_alloc_by_compnt_cross=np.zeros((blend_dim_n,blend_dim_z,blend_dim_j,blend_dim_c))\n",
    "        #tensors with metal group as rows, AE/total mass as col, compont as channel1, vehi_type as channel2\n",
    "        blend_dim_y=TD_matrix.shape[0]\n",
    "        #virgin_bulk_metal_mass_by_compnt_cross=np.zeros((blend_dim_y,1,blend_dim_j,blend_dim_c))\n",
    "        IntAlloy_leftover_by_compnt_cross=np.zeros((blend_dim_y,blend_dim_z,blend_dim_j,blend_dim_c))\n",
    "        #other tensors                            \n",
    "        virgin_AE_mass_by_compnt_cross=np.zeros((1,blend_dim_z,blend_dim_j,blend_dim_c)) \n",
    "        IntAlloy_mass_alloc_by_compnt_cross=np.zeros((blend_dim_y,1,blend_dim_j,blend_dim_c))\n",
    "\n",
    "                \n",
    "        \"\"\"generate outputs from Blending step\"\"\"\n",
    "        for vehi_id in range (blend_dim_c):\n",
    "            IntAlloy_mass_alloc_by_compnt_cross_temp,virgin_alloy_offset_by_compnt_cross_temp,virgin_alloy_needed_by_compnt_cross_temp,virgin_AE_mass_by_compnt_cross_temp,IntAlloy_leftover_by_compnt_cross_temp,Int_Target_mass_alloc_by_compnt_cross_temp,Int_Target_AE_alloc_by_compnt_cross_temp=Blending(IntAlloy_by_compnt_cross[:,:,:,vehi_id],TargetAlloy_mass_by_compnt_by_vehi[:,:,:,vehi_id],\n",
    "                                                                AE_matrix,alloy_group_cutoff,ingot_alloy_loss_matrix,alpha,num_iter,epsilon,total_utility_score_goal,closed_loop_recycl_rate_list,IntAlloy_mass_alloc='Euclidean distance',dilution_explicit=dilution_explicit)\n",
    "\n",
    "            #assign temp values to the intermdiate tensors\n",
    "            IntAlloy_mass_alloc_by_compnt_cross[:,:,:,vehi_id]=IntAlloy_mass_alloc_by_compnt_cross_temp\n",
    "            virgin_alloy_offset_by_compnt_cross[:,:,:,vehi_id]=virgin_alloy_offset_by_compnt_cross_temp\n",
    "            virgin_alloy_needed_by_compnt_cross[:,:,:,vehi_id]=virgin_alloy_needed_by_compnt_cross_temp\n",
    "            #virgin_bulk_metal_mass_by_compnt_cross[:,:,:,vehi_id]=virgin_bulk_metal_mass_by_compnt_cross_temp\n",
    "            virgin_AE_mass_by_compnt_cross[:,:,:,vehi_id]=virgin_AE_mass_by_compnt_cross_temp\n",
    "            IntAlloy_leftover_by_compnt_cross[:,:,:,vehi_id]=IntAlloy_leftover_by_compnt_cross_temp #this should show a decreasing trend along vehicle type id\n",
    "            Int_Target_AE_alloc_by_compnt_cross[:,:,:,vehi_id]=Int_Target_AE_alloc_by_compnt_cross_temp\n",
    "            Int_Target_mass_alloc_by_compnt_cross[:,:,:,vehi_id]=Int_Target_mass_alloc_by_compnt_cross_temp            \n",
    "  \n",
    "\n",
    "        ##Assign final outputs: 4D tensors\n",
    "        IntAlloy_mass_alloc_by_compnt_output=IntAlloy_mass_alloc_by_compnt_cross\n",
    "        virgin_alloy_offset_by_compnt_output=virgin_alloy_offset_by_compnt_cross\n",
    "        virgin_alloy_needed_by_compnt_output=virgin_alloy_needed_by_compnt_cross\n",
    "        #virgin_bulk_metal_mass_by_compnt_output=virgin_bulk_metal_mass_by_compnt_cross\n",
    "        virgin_AE_mass_by_compnt_output=virgin_AE_mass_by_compnt_cross\n",
    "        IntAlloy_leftover_by_compnt_output=IntAlloy_leftover_by_compnt_cross\n",
    "        Int_Target_AE_alloc_by_compnt_output=Int_Target_AE_alloc_by_compnt_cross\n",
    "        Int_Target_mass_alloc_by_compnt_output=Int_Target_mass_alloc_by_compnt_cross\n",
    "\n",
    " \n",
    "    else:\n",
    "        ##have to use a for loop to go through each vehicle type and sum up the variable of interest in the end\n",
    "        \n",
    "        ##====Shredding====##\n",
    "        ##create a 4D tensor to hold all the shreded scraps of all vehicle types\n",
    "        shred_dim_n,shred_dim_z,shred_dim_j,shred_dim_c=(AE_matrix.shape[0],AE_matrix.shape[1],\n",
    "                                                        scrap_compnt_to_SepCom_tensor.shape[1],scrap_compnt_to_SepCom_tensor.shape[2])\n",
    "        shredded_alloys_mix_by_compnt_SepCom_no_cross=np.zeros((shred_dim_n,shred_dim_z,shred_dim_j,shred_dim_c))\n",
    "        shred_loss_to_others_SepCom_no_cross=np.zeros((shred_dim_n,shred_dim_z,shred_dim_j,shred_dim_c))\n",
    "        shredded_alloys_mix_by_compnt_hulk_no_cross=np.zeros((shred_dim_n,shred_dim_z,shred_dim_j,shred_dim_c))\n",
    "        shred_loss_to_others_hulk_no_cross=np.zeros((shred_dim_n,shred_dim_z,shred_dim_j,shred_dim_c))\n",
    "\n",
    "        for vehi_type_id in range(scrap_compnt_to_SepCom_tensor.shape[2]):\n",
    "            #for dismantled compnts\n",
    "            shredded_alloys_mix_by_compnt_SepCom,shred_loss_to_others_SepCom,_=Shredding(scrap_compnt_to_SepCom_tensor[:,:,vehi_type_id],AE_matrix,shredding_loss_rate,cross_mixing=cross_mixing_flag)\n",
    "            shredded_alloys_mix_by_compnt_SepCom_no_cross[:,:,:,vehi_type_id]=shredded_alloys_mix_by_compnt_SepCom[:,:,:]\n",
    "            shred_loss_to_others_SepCom_no_cross[:,:,:,vehi_type_id]=shred_loss_to_others_SepCom[:,:,:]\n",
    "            #for hulk\n",
    "            shredded_alloys_mix_by_compnt_hulk,shred_loss_to_others_hulk,_=Shredding(scrap_compnt_to_hulk_tensor[:,:,vehi_type_id],AE_matrix,shredding_loss_rate,cross_mixing=cross_mixing_flag)\n",
    "            shredded_alloys_mix_by_compnt_hulk_no_cross[:,:,:,vehi_type_id]=shredded_alloys_mix_by_compnt_hulk[:,:,:]\n",
    "            shred_loss_to_others_hulk_no_cross[:,:,:,vehi_type_id]=shred_loss_to_others_hulk[:,:,:]            \n",
    "          \n",
    "        ##====Sorting&remelting====##\n",
    "        ##create a 4D tensor to hold all the sorted&remelted scraps of all vehicle types\n",
    "        sr_dim_y,sr_dim_z,sr_dim_j,sr_dim_c=(TD_matrix.shape[0],TD_matrix.shape[1],shred_dim_j,shred_dim_c)\n",
    "        IntAlloy_by_compnt_no_cross=np.zeros((sr_dim_y,sr_dim_z,sr_dim_j,sr_dim_c))\n",
    "        remelt_loss_to_others_no_cross=np.zeros((sr_dim_y,sr_dim_z,sr_dim_j,sr_dim_c))\n",
    "        to_be_remelted_copy_no_cross=np.zeros((sr_dim_y,sr_dim_z,sr_dim_j,sr_dim_c))\n",
    "        \n",
    "        for vehi_type_id in range(sr_dim_c): #input to be sorted is a 4D tensor\n",
    "            IntAlloy_by_compnt,remelt_loss_to_others,to_be_remelted_copy=Sorting_and_remelting(shredded_alloys_mix_by_compnt_SepCom_no_cross[:,:,:,vehi_type_id],alloy_group_cutoff,\n",
    "                                                                                                             TD_matrix,rand_sort_error=rand_sort_error)\n",
    "            IntAlloy_by_compnt_no_cross[:,:,:,vehi_type_id]=IntAlloy_by_compnt[:,:,:]\n",
    "            remelt_loss_to_others_no_cross[:,:,:,vehi_type_id]=remelt_loss_to_others[:,:,:]\n",
    "            to_be_remelted_copy_no_cross[:,:,:,vehi_type_id]=to_be_remelted_copy[:,:,:]\n",
    "        \n",
    "        ##====Blending====##\n",
    "        \"\"\"similar to Sorting&remelting, only separated compnts are investigated here\"\"\"\n",
    "        ##create 4D tensors to hold all the blending results of scraps of all vehicle types\n",
    "        #tensors with alloys as rows, AE/total mass as col, compont as channel1, vehi_type as channel2\n",
    "        blend_dim_n,blend_dim_z,blend_dim_j,blend_dim_c=(shred_dim_n,shred_dim_z,shred_dim_j,shred_dim_c)\n",
    "        Int_Target_mass_alloc_by_compnt_no_cross=np.zeros((blend_dim_n,1,blend_dim_j,blend_dim_c))\n",
    "        virgin_alloy_needed_by_compnt_no_cross=np.zeros((blend_dim_n,1,blend_dim_j,blend_dim_c))\n",
    "        virgin_alloy_offset_by_compnt_no_cross=np.zeros((blend_dim_n,1,blend_dim_j,blend_dim_c))\n",
    "        Int_Target_AE_alloc_by_compnt_no_cross=np.zeros((blend_dim_n,blend_dim_z,blend_dim_j,blend_dim_c))\n",
    "        #tensors with metal group as rows, AE/total mass as col, compont as channel1, vehi_type as channel2\n",
    "        blend_dim_y,blend_dim_z,blend_dim_j,blend_dim_c=(sr_dim_y,sr_dim_z,sr_dim_j,sr_dim_c)\n",
    "        #virgin_bulk_metal_mass_by_compnt_no_cross=np.zeros((blend_dim_y,1,blend_dim_j,blend_dim_c))\n",
    "        IntAlloy_leftover_by_compnt_no_cross=np.zeros((blend_dim_y,blend_dim_z,blend_dim_j,blend_dim_c))\n",
    "        #other tensors                            \n",
    "        virgin_AE_mass_by_compnt_no_cross=np.zeros((1,blend_dim_z,blend_dim_j,blend_dim_c))   \n",
    "        IntAlloy_mass_alloc_by_compnt_no_cross=np.zeros((blend_dim_y,1,blend_dim_j,blend_dim_c))\n",
    "        \n",
    "        for vehi_type_id in range(blend_dim_c):\n",
    "            IntAlloy_mass_alloc_by_compnt,virgin_alloy_offset_by_compnt,virgin_alloy_needed_by_compnt,virgin_AE_mass_by_compnt,IntAlloy_leftover_by_compnt,Int_Target_mass_alloc_by_compnt,Int_Target_AE_alloc_by_compnt=Blending(IntAlloy_by_compnt_no_cross[:,:,:,vehi_type_id],TargetAlloy_mass_by_compnt_by_vehi[:,:,:,vehi_type_id],\n",
    "                                                                                                                                                                                                 AE_matrix,alloy_group_cutoff,ingot_alloy_loss_matrix,alpha,num_iter,epsilon,total_utility_score_goal,closed_loop_recycl_rate_list,IntAlloy_mass_alloc='Euclidean distance',dilution_explicit=dilution_explicit)\n",
    "            IntAlloy_mass_alloc_by_compnt_no_cross[:,:,:,vehi_type_id]=IntAlloy_mass_alloc_by_compnt[:,:,:]\n",
    "            virgin_alloy_offset_by_compnt_no_cross[:,:,:,vehi_type_id]=virgin_alloy_offset_by_compnt[:,:,:]\n",
    "            virgin_alloy_needed_by_compnt_no_cross[:,:,:,vehi_type_id]=virgin_alloy_needed_by_compnt[:,:,:]\n",
    "            #virgin_bulk_metal_mass_by_compnt_no_cross[:,:,:,vehi_type_id]=virgin_bulk_metal_mass_by_compnt[:,:,:]\n",
    "            virgin_AE_mass_by_compnt_no_cross[:,:,:,vehi_type_id]=virgin_AE_mass_by_compnt[:,:,:]\n",
    "            IntAlloy_leftover_by_compnt_no_cross[:,:,:,vehi_type_id]=IntAlloy_leftover_by_compnt[:,:,:]\n",
    "            Int_Target_AE_alloc_by_compnt_no_cross[:,:,:,vehi_type_id]=Int_Target_AE_alloc_by_compnt[:,:,:]\n",
    "            Int_Target_mass_alloc_by_compnt_no_cross[:,:,:,vehi_type_id]=Int_Target_mass_alloc_by_compnt[:,:,:]\n",
    "        \n",
    "        \n",
    "        ##Assign final outputs: 4D tensors\n",
    "        IntAlloy_mass_alloc_by_compnt_output=IntAlloy_mass_alloc_by_compnt_no_cross       \n",
    "        virgin_alloy_offset_by_compnt_output=virgin_alloy_offset_by_compnt_no_cross\n",
    "        virgin_alloy_needed_by_compnt_output=virgin_alloy_needed_by_compnt_no_cross\n",
    "        #virgin_bulk_metal_mass_by_compnt_output=virgin_bulk_metal_mass_by_compnt_no_cross\n",
    "        virgin_AE_mass_by_compnt_output=virgin_AE_mass_by_compnt_no_cross\n",
    "        IntAlloy_leftover_by_compnt_output=IntAlloy_leftover_by_compnt_no_cross\n",
    "        Int_Target_AE_alloc_by_compnt_output=Int_Target_AE_alloc_by_compnt_no_cross\n",
    "        Int_Target_mass_alloc_by_compnt_output=Int_Target_mass_alloc_by_compnt_no_cross\n",
    "        \n",
    "\n",
    "    \n",
    "    return IntAlloy_mass_alloc_by_compnt_output,virgin_alloy_offset_by_compnt_output,virgin_alloy_needed_by_compnt_output,virgin_AE_mass_by_compnt_output,IntAlloy_leftover_by_compnt_output,Int_Target_mass_alloc_by_compnt_output,Int_Target_AE_alloc_by_compnt_output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCA_MOD for multiple components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LCA_MOD_multi(PIOT_A,ES,eff,EF,y_time_series_tensor,EO_sheet_final_extended,eff_consider=True):\n",
    "    \"\"\"This module calculate the inventory and emissions for products from primary and secondary manufacturing and EOL stages\n",
    "    ## Input variables\n",
    "    -PIOT_A: a matrix of intersectoral coefficients; (n,n)\n",
    "    -ES: a matrix of energy source share (in fraction) for each sector; (m,n)\n",
    "    -eff: energy use efficiency for each energy source; (m,1)\n",
    "    -eff_consider: a boolean variable determining whether eff vector will be used in the calculation, set to False by default\n",
    "    -EF: emission factors for each energy source; (1,m)\n",
    "    -y_time_series_tensor: used for multiple purposes (below)\n",
    "        a 4D tensor that contains materials (n) by component (j) for each vehicle type (channel 1=c), for each year (channel 2=t) \n",
    "        # for virgin alloys: a 4D tensor that contains virgin alloys demand (after offset) for each component (j) and vehicle type (c) for each year (t), in mass (kg), (ny,j,c,t)\n",
    "            although the row# is n=ny, only the first nc rows (correspond to virgin alloys) may have postive values, the rest are zeros\n",
    "        # for virgin bulk: a 4D tensor containing virgin bulk metals (row), for each component OR car hulk (j) for each vehicle type (c) for each year (t), in mass (kg), (ny,j,c,t)\n",
    "            although the row# is n=ny, only the designated rows (corresponds to bulk metal primary production) may have positive values, the rest are zeros \n",
    "        # for virgin AE: a 4D tensor containing virgin AEs needed (row), for each component OR car hulk (j) fo each vehicle type (c) for each year (t), in mass (kg), (ny,j,c,t)\n",
    "            although the row# is n=ny, only the designated rows (corresponds to AE primary production) may have positive values, the rest are zeros\n",
    "        # for secondary alloys (blended, on-spec, to displace virgin ones): a 4D tensor that contains alloys (from secondary source) for each component (j) for each vehicle type (c) for each year (t), in mass (kg), (ny,j,c,t)\n",
    "            although the row# is n=ny, only the second nc rows (correspond to alloys from secondary sources) may have positive values, the rest are zeros\n",
    "        \n",
    "    -EO_sheet_final_extended: a dataframe with col_name being materials sectors and row being process energy data (max or min) for each sector\n",
    "                                in the shape of (1, n)   \n",
    "    ## Output variables\n",
    "    *may need to convert tensors into dicts\n",
    "    -total_energy_all_years_dict: total energy consumption (in MJ), in the format of {'year1':[emission1,emission2...]...}\n",
    "    -total_energy_by_source_all_years_dict: total energy consumption by source, in the format of {'year1':df1(total_energy_by source)...}\n",
    "    -total_emissions_all_years_dict: total carbon emission, in the format of {'year1':[emission1,emission2...]...}\n",
    "    -total_emissions_by_source_all_years_dict\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"update variables for material and energy inventory calculation\"\"\"\n",
    "    ##calculate total output for material demand for each year  \n",
    "    #the x_tensor contains: material sector (n) by component (j) by vehicle type (c) by year (t)\n",
    "    Leontief=(np.identity(len(PIOT_A.index))-PIOT_A)\n",
    "    temp_inv=pd.DataFrame(data=inv(Leontief),index=PIOT_A.index,columns=PIOT_A.index) #(n,n)\n",
    "    #note here we use 'm' instead of 'n' to respresent the 2nd axis, as you are not allowed to use the same letter to represent different axis for an tensor\n",
    "    #even when both axes have the same dimension (both n material sectors), so accordingly the 1st axis of 2nd tensor is also representd by 'm'\n",
    "\n",
    "    #print (\"Mg demand (y) for all compont in light PHEV: \", np.sum(y_time_series_tensor[145,:,3,:]))\n",
    "    \n",
    "    total_output_x_tensor=np.einsum('nm,mjct->njct',temp_inv,y_time_series_tensor)\n",
    "    #set all negative values to zero\n",
    "    total_output_x_tensor[total_output_x_tensor<0]=0\n",
    "\n",
    "    #show sum of individual AE\n",
    "    print (\"total Al for ALL compnt in light PHEV: \",np.sum(total_output_x_tensor[140,:,3,:]) )\n",
    "    print (\"total Si for ALL compnt in light PHEV: \",np.sum(total_output_x_tensor[141,:,3,:]) )\n",
    "    print (\"total Fe for ALL compnt in light PHEV: \",np.sum(total_output_x_tensor[142,:,3,:]) )\n",
    "    print (\"total Cu for ALL compnt in light PHEV: \",np.sum(total_output_x_tensor[143,:,3,:]) )\n",
    "    print (\"total Mn for ALL compnt in light PHEV: \",np.sum(total_output_x_tensor[144,:,3,:]) )\n",
    "    print (\"total Mg for ALL compnt in light PHEV: \",np.sum(total_output_x_tensor[145,:,3,:]) )\n",
    "    print (\"total Cr for ALL compnt in light PHEV: \",np.sum(total_output_x_tensor[147,:,3,:]) )\n",
    "    print (\"total Ni for ALL compnt in light PHEV: \",np.sum(total_output_x_tensor[148,:,3,:]) )\n",
    "    print (\"total Zn for ALL compnt in light PHEV: \",np.sum(total_output_x_tensor[149,:,3,:]) )\n",
    "    print (\"total Ti for ALL compnt in light PHEV: \",np.sum(total_output_x_tensor[150,:,3,:]) )\n",
    "    print (\"total Mo for ALL compnt in light PHEV: \",np.sum(total_output_x_tensor[151,:,3,:]) )\n",
    "    print (\"total C for ALL compnt in light PHEV: \",np.sum(total_output_x_tensor[152,:,3,:]) )\n",
    "    \n",
    "    ##calculate the normalized energy consumption per kg material, for all possible combinations of EO (m,n,x)\n",
    "    energy_by_source_normal_tensor=np.einsum('mn,xn->mnx',ES,EO_sheet_final_extended)\n",
    "    if eff_consider:\n",
    "        energy_by_source_normal_tensor=np.einsum('mnx,ml->mnx', energy_by_source_normal_tensor,eff)\n",
    "\n",
    "    ##scale up from normalized values to total values for each energy source (m), using the total output total_output_x_tensor, for vehicle type (c), all EO combos (x) and years (t); (m,c,x,t)\n",
    "    total_energy_by_source_all_years_tensor=np.einsum('mnx,njct->mcxt',energy_by_source_normal_tensor,total_output_x_tensor)\n",
    "\n",
    "    ##calculation emissions (m,c,x,t)\n",
    "    total_emissions_by_source_all_years_tensor=np.einsum('lm,mcxt->mcxt', EF,total_energy_by_source_all_years_tensor)\n",
    "    \n",
    "    \n",
    "    return total_emissions_by_source_all_years_tensor,total_energy_by_source_all_years_tensor\n",
    "\n",
    " \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wrapper_MOD(MFA_time_series_file,scenario_num,metal_process_data_workbook,body_composition_workbook,time_span_tuple,compnt_loc_tuple_list,\n",
    "                v_s_tensor,shredding_loss_rate,alloy_group_cutoff,\n",
    "                AE_row_list,Mg_Pigd_share_dict,remelt_scrap_row_list,\n",
    "                alpha,num_iter,epsilon,total_utility_score_goal,closed_loop_recycl_rate_list,max_or_min,dilution_explicit,\n",
    "                rand_sort_error=0.02,cross_mixing_flag=True,**kwargs):\n",
    "    \"\"\"This module configures the rest of the modules to perform the investigation of interest\n",
    "    ## Input variables (excluding intermediate inputs from other modules)\n",
    "    ***The input variables below are for MFA module***\n",
    "    -MFA_time_series_file: a workbook contains data sheets of time series data\n",
    "    -scenario_num: scenario number that reflects different assumptions regarding the MFA scenarios\n",
    "    ***The input variables below are for Data parsing module***\n",
    "    -metal_process_data_workbook: contains worksheets of \"Primary\", \"Manu\" and \"EoL treatment\" that contain process data\n",
    "    -body_composition_workbook: contains worksheet of the vehicle body composition data\n",
    "    -compnt_loc_tuple_list: store the start and end rows (both inclusive) of a compnt in material composition tab \"Alloy_compnt_G&L\"\n",
    "    -time_span_tuple: a tuple contains the starting and ending years of the time span for modeling, identical to what's specified in MFA module    \n",
    "    -max_or_min: a boolean variable to decide whether use the max or min values of EO for subsequent calculations\n",
    "    ***The input variables below are for EoL module***\n",
    "        **for Dismantling**\n",
    "        -[intermediate variable now] v_c_tensor: a 3D tensor contains vectors containing the mass of components (j) of a specific type of vehicle (1) for c types, in total mass (kg), (j,1,c)\n",
    "        -v_s_tensor: a 3D tensor contains vectors containing the dismantling rate of each component (rest is left to \"car hulk\"), corresponding to v_c_tensor, in %, (j,1,c)\n",
    "            note that \"reuse/remanufacturing\" (e.g., remanu of engine) is not considered; \"dismantling rate\" strictly refers to the portion that is \"unwanted and shredded separately (from other components or hulk)\"\n",
    "        -[intermediate variable now] mat_compnt_tensor: materials (row) by components (col) matrix that shows the material composition of each component, \n",
    "           in normalized mass (kg/kg), (n,j), for all vehicle types (c), (n,j,c)\n",
    "        **for Shredding**\n",
    "        -[intermediate variable now] AE_matrix: alloys (row) by alloying elements (col) matrix that shows the AE composition of each alloy, in %, (n,z)\n",
    "        -shredding_loss_rate: a list contains the shredding loss rate for each component (len=j) or car hulk (len=1)\n",
    "        **for Sorting&Remelting**\n",
    "        -[intermediate variable now] TD_matrix: thermodynmic limit matrix that contains the yield of AE (col) during remelting of scrap allys of same group (row),\n",
    "            in %, (y,z)\n",
    "        -rand_sort_error: a random sorting error rate (scalar) that indicates the incidental \n",
    "        -alloy_group_cutoff: a list contains tuple of (staring,ending) rows in the alloy-related matrix that belongs to a certain common metal group,\n",
    "            len()= #of metal types = y\n",
    "        **for Blending**\n",
    "        -[intermediate variable now] TargetAlloy_mass_by_compnt:a 3D tensor that contains target alloy by metal (row), by AE composition (col), \n",
    "            for each component OR car hulk (channel), in total mass (kg), (n,z,j)\n",
    "        -dilution_explicit: a boolean variable to indicate whether \"dilution (quality loss)\" is explicitly modeled\n",
    "        -closed_loop_recycl_rate_list: a list containing the metal group-lvl closed-loop recycling rate factors, in %\n",
    "        -alpha: learning rate, scalar (dim=0)\n",
    "        -num_iter: number of iterations before the gradient descent algorithm is terminated, scalar (dim=0)\n",
    "        -epsilon: a small value to create margins of AE_spec%, in %\n",
    "        -total_utility_score_goal: a small value to compare with total utility score of an iteration as the termination criteron , scalar (dim=0)\n",
    "\n",
    "    ***The input variables below are for other sections of the code in Wrapper MOD***\n",
    "    -AE_row_list: a list of row numbers that correspond to the location of AE entries\n",
    "    -Mg_Pigd_share_dict: a dict contains row# of Mg from Pigdeon (key) in full length y vector and its market share (value), {row#: %Pigd}\n",
    "    -remelt_scrap_row_list: a list of row numbers that correspond to the location of scrap remelted entries\n",
    "    \n",
    "    ## Output variables (excluding intermediate outputs from other modules)\n",
    "    -\n",
    "    \n",
    "    ###KEYWORDS for NAMING CONVENTION\n",
    "    #Dimenions\n",
    "    'n': typically refer to the dimension of \"alloys\"\n",
    "        sometimes also use \"nc\" (<n) to refer to alloy products that used in compnts (exluding intermediate alloys)\n",
    "    'j': typically refere to the dimenion of \"compnts\"\n",
    "    'z': typically refere to the dimenion of \"AEs\"\n",
    "    'y': typically refere to the dimenion of \"metal groups\"\n",
    "    'c': typically refere to the dimenion of \"vehicle types\"\n",
    "    't': typically refere to the dimenion of \"time (year)\"\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"Start with MFA to generate flow data of vehicles\"\"\"\n",
    "    _,num_new_vehicles_dict,num_retired_vehicles_dict=MFA_MOD(MFA_time_series_file,scenario_num)\n",
    "  \n",
    "    \n",
    "    \"\"\"Run the flow data through Data parsing module\"\"\"\n",
    "    total_new_vehi_time_series_tensor,TD_matrix,ingot_alloy_loss_matrix,dim_ny, mat_compnt_tensor, alloy_AE_spec_matrix, data_body_composition_parsed,EO_sheet_final_extended,y_total_alloy_time_series_tensor,y_total_scrap_potential_time_series_tensor,PIOT_A_sheet,ES_sheet,eff_sheet,EF_sheet=Data_parsing_MOD(metal_process_data_workbook,body_composition_workbook,num_new_vehicles_dict,num_retired_vehicles_dict,time_span_tuple,compnt_loc_tuple_list,max_or_min)\n",
    "    \n",
    "    \n",
    "    ##create the v_c_year_tensor (4D): convert from y_total_scrap_potential_time_series_tensor (n,j,c,t)\n",
    "    #CAUTION: the rows (n>nc) in y_total_scrap_potential_time_series_tensor contains NOT ONLY scrap alloys (where you have values)\n",
    "    #         BUT ALSO the rows of intermediate products and virgin alloys (but you should have all zeros there)\n",
    "    temp_convert_vc=np.sum(y_total_scrap_potential_time_series_tensor,axis=0,keepdims=True) #(1,j,c,t)\n",
    "    v_c_year_tensor=np.einsum('ljct->jlct',temp_convert_vc)\n",
    "\n",
    "    ##create the TargetAlloy_mass_by_compnt_year_tensor(4D): convert from y_total_alloy_time_series_tensor (n,j,c,t)\n",
    "    #First, create a new dimension to expand mass of alloy to mass of AE: (n,j,c,t)—>(n,z,j,c,t)\n",
    "    temp_convert_TA=np.expand_dims(y_total_alloy_time_series_tensor[:alloy_AE_spec_matrix.shape[0],:,:,:],axis=1) #(n,1,j,c,t)\n",
    "    #then mulply mass of alloy with its corresponding spec\n",
    "    temp_convert_TA=np.einsum('nljct,nz->nzjct', temp_convert_TA,alloy_AE_spec_matrix) #(n,z,j,c,t)\n",
    "\n",
    "    TargetAlloy_mass_by_compnt_by_vehi_by_year_tensor=temp_convert_TA #this keeps the 'vehi_type' dimension (c) for the target alloys\n",
    "      \n",
    "    ##prepare other relevant input variables to EoL\n",
    "    AE_matrix=alloy_AE_spec_matrix\n",
    "    \n",
    "    \n",
    "    \"\"\"Run the EoL module in a FOR loop to generate the virgin material demand (after offset) and treatment inventory for each year\"\"\"\n",
    "    ##prepare the output variables for EoL module\n",
    "    #material flows\n",
    "    dim_nc,dim_j,dim_y,dim_t,dim_z=(alloy_AE_spec_matrix.shape[0],mat_compnt_tensor.shape[1],len(alloy_group_cutoff),y_total_alloy_time_series_tensor.shape[3],alloy_AE_spec_matrix.shape[1])\n",
    "    \n",
    "    #create a dimension for vehicle type\n",
    "    dim_c=v_s_tensor.shape[2]\n",
    "    \"\"\"all the output variables (below) are updated to include a vehicle type dimension\"\"\"\n",
    "    virgin_alloy_needed_by_compnt_year=np.zeros((dim_nc,1,dim_j,dim_c,dim_t))\n",
    "    virgin_alloy_offset_by_compnt_year=np.zeros((dim_nc,1,dim_j,dim_c,dim_t))\n",
    "    #virgin_bulk_metal_mass_by_compnt_year=np.zeros((dim_y,1,dim_j,dim_c,dim_t))\n",
    "    IntAlloy_mass_alloc_by_compnt_year=np.zeros((dim_y,1,dim_j,dim_c,dim_t))\n",
    "    virgin_AE_mass_by_compnt_year=np.zeros((1,dim_z,dim_j,dim_c,dim_t))\n",
    "    IntAlloy_leftover_by_compnt_year=np.zeros((dim_y,dim_z,dim_j,dim_c,dim_t))\n",
    "    Int_Target_AE_alloc_by_compnt_year=np.zeros((dim_nc,dim_z,dim_j,dim_c,dim_t))\n",
    "    Int_Target_mass_alloc_by_compnt_year=np.zeros((dim_nc,1,dim_j,dim_c,dim_t))\n",
    " \n",
    "    \n",
    "    ##loop through the time span\n",
    "    for year_index in range (time_span_tuple[1]-time_span_tuple[0]+1): #starts with year0, until yearX\n",
    "        v_c_tensor=v_c_year_tensor[:,:,:,year_index]\n",
    "        \"\"\"need to carry the vehicle_type dimension\"\"\"\n",
    "        TargetAlloy_mass_by_compnt_by_vehi=TargetAlloy_mass_by_compnt_by_vehi_by_year_tensor[:,:,:,:,year_index]\n",
    "        \n",
    "        IntAlloy_mass_alloc_by_compnt_output,virgin_alloy_offset_by_compnt_output,virgin_alloy_needed_by_compnt_output,virgin_AE_mass_by_compnt_output,IntAlloy_leftover_by_compnt_output,Int_Target_mass_alloc_by_compnt_output,Int_Target_AE_alloc_by_compnt_output=EoL_MOD(v_c_tensor,v_s_tensor,mat_compnt_tensor,AE_matrix,shredding_loss_rate,TD_matrix,\n",
    "                                                                                                                                                                                                                                alloy_group_cutoff,TargetAlloy_mass_by_compnt_by_vehi,ingot_alloy_loss_matrix,alpha,num_iter,epsilon,total_utility_score_goal,closed_loop_recycl_rate_list,dilution_explicit=dilution_explicit,rand_sort_error=rand_sort_error,cross_mixing_flag=cross_mixing_flag)\n",
    "\n",
    "        ##populate the outputs of the Wrapper\n",
    "        #material flows\n",
    "        IntAlloy_mass_alloc_by_compnt_year[:,:,:,:,year_index]=IntAlloy_mass_alloc_by_compnt_output[:,:,:,:]\n",
    "        virgin_alloy_offset_by_compnt_year[:,:,:,:,year_index]=virgin_alloy_offset_by_compnt_output[:,:,:,:]\n",
    "        virgin_alloy_needed_by_compnt_year[:,:,:,:,year_index]=virgin_alloy_needed_by_compnt_output[:,:,:,:]\n",
    "        #virgin_bulk_metal_mass_by_compnt_year[:,:,:,:,year_index]=virgin_bulk_metal_mass_by_compnt_output[:,:,:,:]\n",
    "        virgin_AE_mass_by_compnt_year[:,:,:,:,year_index]=virgin_AE_mass_by_compnt_output[:,:,:,:]\n",
    "        IntAlloy_leftover_by_compnt_year[:,:,:,:,year_index]=IntAlloy_leftover_by_compnt_output[:,:,:,:]\n",
    "        Int_Target_AE_alloc_by_compnt_year[:,:,:,:,year_index]=Int_Target_AE_alloc_by_compnt_output[:,:,:,:]\n",
    "\n",
    "\n",
    "        \n",
    "    \"\"\"reformat the outputs to have ny rows so that they can be fed to LCA module\"\"\"\n",
    "    ###need to \"place“ the output tensors to the corresponding position of the tensor with ny rows###\n",
    "    ##create a set of \"zero tensor\" with desired dimensions (e.g., change dim_nc to dim_ny that corresponds to the # of rows for y vectors)\n",
    "    zeros_virgin_alloy_needed_by_compnt_year=np.zeros((dim_ny,1,dim_j,dim_c,dim_t)) #need to feed in numbers for the first dim_nc rows\n",
    "    #zeros_virgin_bulk_metal_mass_by_compnt_year=np.zeros((dim_ny,1,dim_j,dim_c,dim_t)) #need to feed in numbers for rows of virgin bulk metals\n",
    "    zeros_virgin_AE_mass_by_compnt_year=np.zeros((dim_ny,1,dim_j,dim_c,dim_t)) #need to feed in number for rows of AEs\n",
    "    zeros_virgin_alloy_offset_by_compnt_year=np.zeros((dim_ny,1,dim_j,dim_c,dim_t)) #need to feed in number for the second dim_nc rows\n",
    "    zeros_IntAlloy_mass_alloc_by_compnt_year=np.zeros((dim_ny,1,dim_j,dim_c,dim_t)) #need to feed in number for rows of scrap remelted\n",
    "    \n",
    "    ##assign values to the output variables to be used for LCA\n",
    "    #for virgin alloy demand\n",
    "    zeros_virgin_alloy_needed_by_compnt_year[0:dim_nc,:,:,:,:]=virgin_alloy_needed_by_compnt_year[:,:,:,:,:]\n",
    "    to_LCA_virgin_alloy_needed_by_compnt_year=zeros_virgin_alloy_needed_by_compnt_year\n",
    "        \n",
    "    #similarly, loop through the row locator for AE entries\n",
    "    #Also make sure the index matches between this list and what's in 'virgin_AE_mass_by_compnt_year' （make sure the order of AEs in this list is the same as the order in column names of AEs)\n",
    "    #re-organize the tensor dimension to fit the dimension of output tensor\n",
    "    temp_virgin_AE_mass_by_compnt_year=np.einsum('lzjct->zljct',virgin_AE_mass_by_compnt_year)\n",
    "    \n",
    "    for index,AE_row in enumerate(AE_row_list):\n",
    "        zeros_virgin_AE_mass_by_compnt_year[AE_row,:,:,:,:]=temp_virgin_AE_mass_by_compnt_year[index,:,:,:,:]\n",
    "    \n",
    "    #similarly, deal with issue of having two Mg primary production technologies\n",
    "    #could be combined with handling of Mg in virgin bulk metal, but leave as separated for clarity\n",
    "    for key, value in Mg_Pigd_share_dict.items():\n",
    "        #actually there should be only one pair of key and value...\n",
    "        #the row of 'electrolytic' should always be above 'Pigdgeon'\n",
    "        temp_Mg_alloc=deepcopy(zeros_virgin_AE_mass_by_compnt_year[key-1,:,:,:,:])*value    \n",
    "        zeros_virgin_AE_mass_by_compnt_year[key,:,:,:,:]=temp_Mg_alloc\n",
    "        #subtract the corresponding amount from row of 'electrolytic' \n",
    "        zeros_virgin_AE_mass_by_compnt_year[key-1,:,:,:,:]-=temp_Mg_alloc\n",
    "\n",
    "    #assign the final result to output tensor (to LCA)\n",
    "    to_LCA_virgin_AE_mass_by_compnt_year=zeros_virgin_AE_mass_by_compnt_year\n",
    "    \n",
    "    #for scrap alloy processing (there are energy associated with using scrap alloys to offset the virgin demand)\n",
    "    zeros_virgin_alloy_offset_by_compnt_year[dim_nc:(dim_nc+dim_nc),:,:,:,:]=virgin_alloy_offset_by_compnt_year[:,:,:,:,:]\n",
    "    to_LCA_virgin_alloy_offset_by_compnt_year=zeros_virgin_alloy_offset_by_compnt_year\n",
    "    \n",
    "    #for remelted scrap\n",
    "    for index,remelt_row in enumerate(remelt_scrap_row_list):\n",
    "        #make sure the sequence of 'remelt_row' matches the sequence of 'remelted scrap' in IntAlloy_mass_alloc_by_compnt_year\n",
    "        zeros_IntAlloy_mass_alloc_by_compnt_year[remelt_row,:,:,:,:]=IntAlloy_mass_alloc_by_compnt_year[index,:,:,:,:]\n",
    "    to_LCA_IntAlloy_mass_alloc_by_compnt_year=zeros_IntAlloy_mass_alloc_by_compnt_year\n",
    "        \n",
    "    ##reformat the output tensors (to LCA_MOD) ensure their dimensions all conform to (ny,j,c,t) for 'y_time_series tensor'\n",
    "    to_LCA_virgin_alloy_needed_by_compnt_year=np.squeeze(to_LCA_virgin_alloy_needed_by_compnt_year)\n",
    "    #to_LCA_virgin_bulk_metal_mass_by_compnt_year=np.squeeze(to_LCA_virgin_bulk_metal_mass_by_compnt_year)\n",
    "    to_LCA_virgin_AE_mass_by_compnt_year=np.squeeze(to_LCA_virgin_AE_mass_by_compnt_year)\n",
    "    to_LCA_virgin_alloy_offset_by_compnt_year=np.squeeze(to_LCA_virgin_alloy_offset_by_compnt_year)\n",
    "    to_LCA_IntAlloy_mass_alloc_by_compnt_year=np.squeeze(to_LCA_IntAlloy_mass_alloc_by_compnt_year)\n",
    "    \n",
    "    \"\"\"run LCA module\"\"\"\n",
    "    ###run the LCA module multiple times for different y_time_series tensors\n",
    "    ##from virgin alloy demand (only from ingot-to-alloy energy consumptions are captured)\n",
    "    for_virgin_alloy_total_emissions_by_source_all_years_tensor,for_virgin_alloy_total_energy_by_source_all_years_tensor=LCA_MOD_multi(PIOT_A_sheet,ES_sheet,eff_sheet,EF_sheet,to_LCA_virgin_alloy_needed_by_compnt_year,EO_sheet_final_extended,eff_consider=True)\n",
    "    \n",
    "    ##from virgin AE demand for 1) blending with scrap ingot, 2) AE that consist of virgin alloys still needed\n",
    "    for_virgin_AE_total_emissions_by_source_all_years_tensor,for_virgin_AE_total_energy_by_source_all_years_tensor=LCA_MOD_multi(PIOT_A_sheet,ES_sheet,eff_sheet,EF_sheet,to_LCA_virgin_AE_mass_by_compnt_year,EO_sheet_final_extended,eff_consider=True)\n",
    "    \n",
    "    ##from making alloy from secondary source\n",
    "    #CAUTION: THIS ONLY COVERS energy consumption from \"Blending\" and beyond (inclusive of \"Blending step\" which is represente d by 'ingotting' step in PIOT)\n",
    "    #i.e., input coefficients to metal ingots are zero in PIOT\n",
    "    for_secondary_alloy_total_emissions_by_source_all_years_tensor,for_secondary_alloy_total_energy_by_source_all_years_tensor=LCA_MOD_multi(PIOT_A_sheet,ES_sheet,eff_sheet,EF_sheet,to_LCA_virgin_alloy_offset_by_compnt_year,EO_sheet_final_extended,eff_consider=True)\n",
    "        \n",
    "    ##from scrap remelting (automatically link 'scrap separation, shredding and dismentling' using PIOT)\n",
    "    #use 'to_LCA_IntAlloy_mass_alloc_by_compnt_year' as demand for scrap remelted\n",
    "    for_scrap_proc_before_ingot_total_emissions_by_source_all_years_tensor,for_scrap_proc_before_ingot_total_energy_by_source_all_years_tensor=LCA_MOD_multi(PIOT_A_sheet,ES_sheet,eff_sheet,EF_sheet,to_LCA_IntAlloy_mass_alloc_by_compnt_year,EO_sheet_final_extended,eff_consider=True)\n",
    "    \n",
    "    ###get an intermdiate sum of total energy and emissions\n",
    "    ##keep original dimension (m,c,x,t), where m is the energy sources, x is the total # of combinations [min,max] values for EO\n",
    "    intermediate_total_LC_energy=for_virgin_alloy_total_energy_by_source_all_years_tensor+for_virgin_AE_total_energy_by_source_all_years_tensor+ \\\n",
    "    for_secondary_alloy_total_energy_by_source_all_years_tensor+for_scrap_proc_before_ingot_total_energy_by_source_all_years_tensor\n",
    "    intermediate_total_LC_emissions=for_virgin_alloy_total_emissions_by_source_all_years_tensor+for_virgin_AE_total_emissions_by_source_all_years_tensor+ \\\n",
    "    for_secondary_alloy_total_emissions_by_source_all_years_tensor+for_scrap_proc_before_ingot_total_emissions_by_source_all_years_tensor\n",
    "\n",
    "    ##reduce the dimension to (c,x,t), then re-order to (x,c,t) so that later you can add additional impacts that has a dimension of (1,c,t)\n",
    "    intermediate_total_LC_energy=np.sum(intermediate_total_LC_energy, axis=0)\n",
    "    intermediate_total_LC_emissions=np.sum(intermediate_total_LC_emissions,axis=0)\n",
    "    intermediate_total_LC_energy=np.einsum('cxt->xct',intermediate_total_LC_energy)\n",
    "    intermediate_total_LC_emissions=np.einsum('cxt->xct',intermediate_total_LC_emissions)\n",
    "    \n",
    "    \n",
    "    \"\"\"account for impacts from other steps (kwargs of dict)\"\"\"\n",
    "    ###the input should be multipliers that scale based on the vehicle production volume\n",
    "    if kwargs is not None:\n",
    "        for kw,value in kwargs.items():\n",
    "            if kw=='assembly_energy_consumption_MJ_vehicle':\n",
    "                #this kw should contain the assembly energy in MJ/vehicle\n",
    "                #create empty tensor for total energy from vehicle assembly (1,c,t)\n",
    "                total_energy_vehi_assembly_year=np.zeros((1,dim_c,dim_t))\n",
    "                #calculate total energy from vehicle assembly\n",
    "                total_energy_vehi_assembly_year=np.multiply(total_new_vehi_time_series_tensor,value)\n",
    "                \n",
    "                ##update total LC energy\n",
    "                intermediate_total_LC_energy+=total_energy_vehi_assembly_year\n",
    "            elif kw=='assembly_emissions_kgCO2eq_vehicle':\n",
    "                #this kw should contain the assembly emission in MJ/vehicle\n",
    "                #create empty tensor for total emissions from vehicle assembly (1,c,t)\n",
    "                total_emissions_vehi_assembly_year=np.zeros((1,dim_c,dim_t)) \n",
    "                #calculate total emissions from vehicle assembly\n",
    "                total_emissions_vehi_assembly_year=np.multiply(total_new_vehi_time_series_tensor,value)\n",
    "                 \n",
    "                ##update total emissions\n",
    "                intermediate_total_LC_emissions+=total_emissions_vehi_assembly_year\n",
    "\n",
    "\n",
    "    \"\"\"Generate final results\"\"\"\n",
    "    ###====material flows by year====### \n",
    "    ##virgin alloy (after offset) demand, AE, bulk (ny,c,t)\n",
    "    final_virgin_alloy_demand=np.sum(to_LCA_virgin_alloy_needed_by_compnt_year,axis=1) #from (ny,j,c,t) to (ny,c,t)\n",
    "    final_AE_demand=np.sum(to_LCA_virgin_AE_mass_by_compnt_year,axis=1) #from (ny,j,c,t) to (ny,c,t)\n",
    "    #final_bulk_demand=np.sum(to_LCA_virgin_bulk_metal_mass_by_compnt_year,axis=1) #from (ny,j,c,t) to (ny,c,t)\n",
    "    \n",
    "    ##leftover (remelted) intermediate scrap alloy (y,c,t)\n",
    "    final_leftover_IntAlloy=np.sum(IntAlloy_leftover_by_compnt_year,axis=1) #from (y,z,j,c,t) to (y,j,c,t)\n",
    "    final_leftover_IntAlloy=np.sum(final_leftover_IntAlloy,axis=1) #from (y,j,c,t) to (y,c,t)\n",
    "    \n",
    "    ##allocated (utilized) intermediate scrap alloy (y,c,t)\n",
    "    final_utilized_IntAlloy=np.squeeze(IntAlloy_mass_alloc_by_compnt_year) #from (y,1,j,c,t) to (y,j,c,t)\n",
    "    final_utilized_IntAlloy=np.sum(final_utilized_IntAlloy,axis=1) #from (y,j,c,t) to (y,c,t)\n",
    "    final_utilized_ratio=np.round(final_utilized_IntAlloy/(final_utilized_IntAlloy+final_leftover_IntAlloy)*100,2) #(y,c,t)\n",
    "    \n",
    "    ##offset (derived from IntAlloy) (nc,c,t)\n",
    "    final_virgin_alloy_offset=np.squeeze(virgin_alloy_offset_by_compnt_year) #from (nc,1,j,c,t) to (nc,j,c,t)\n",
    "    final_virgin_alloy_offset=np.sum(final_virgin_alloy_offset,axis=1) #from (nc,j,c,t) to (nc,c,t)\n",
    "    \n",
    "    ##virgin alloy demand after offset (nc,c,t)\n",
    "    final_virgin_alloy_needed=np.squeeze(virgin_alloy_needed_by_compnt_year) #from (nc,1,j,c,t) to (nc,j,c,t)\n",
    "    final_virgin_alloy_needed=np.sum(final_virgin_alloy_needed,axis=1) #from (nc,j,c,t) to (nc,c,t)\n",
    "    \n",
    "    ###====energy consumption by year (x,c,t)====###\n",
    "    ##total life cycle energy consumption by vehicle type\n",
    "    final_total_LC_energy=intermediate_total_LC_energy\n",
    "    #print \"final_total_LC_energy: \",final_total_LC_energy,\"\\n\"\n",
    "    \n",
    "    ##normalized life cycle energy consumption by vehicle type (x,c,t)\n",
    "    final_norm_LC_energy=np.divide(final_total_LC_energy,total_new_vehi_time_series_tensor) #dim=(x,c,t)/(1,c,t)\n",
    "    \n",
    "    ###====emissions by year (x,c,t)====###\n",
    "    ##total life cycle emissions by vehicle type\n",
    "    final_total_LC_emissions=intermediate_total_LC_emissions\n",
    "    \n",
    "    ##normalized life cycle emissions by vehicle type\n",
    "    final_norm_LC_emissions=np.divide(final_total_LC_emissions,total_new_vehi_time_series_tensor) #dim=(x,c,t)/(1,c,t)\n",
    "    \n",
    "    ###secondary material reuse rate (y,c,t): e.g., %of Al alloy demand is by using scrap alloys (mass ratio)\n",
    "    \n",
    "    \n",
    "    \"\"\"test output zone\"\"\"\n",
    "    #print \"normalized LC energy (MJ/vehicle): \",final_norm_LC_energy,\"\\n\"\n",
    "    #print \"mean of normalized LC energy (MJ/vehicle): \",np.mean(final_norm_LC_energy,axis=0),\"\\n\"\n",
    "    #print \"dim of normalized LC energy (MJ/vehicle): \",final_norm_LC_energy.shape,\"\\n\"\n",
    "    \n",
    "    ###material flows\n",
    "    #print \"utilized IntAlloy (all vehicles):\",np.sum(final_utilized_IntAlloy,axis=1,keepdims=True),\"\\n\" #(y,1,t),collapse overall all vehicle types and components\n",
    "    #print \"utilized IntAlloy (by vehicle type):\",final_utilized_IntAlloy,\"\\n\" #(y,c,t)  \n",
    "    #print \"dim of utilized IntAlloy: \", np.sum(final_utilized_IntAlloy,axis=1,keepdims=True).shape,\"\\n\"\n",
    "    #print \"leftover IntAlloy by mass (all vehicles): \", np.sum(final_leftover_IntAlloy,axis=1,keepdims=True),\"\\n\" #(y,1,t),collapse overall all vehicle types and components\n",
    "    #print \"leftover IntAlloy by mass (by vehicle type): \", final_leftover_IntAlloy,\"\\n\"     \n",
    "    #print \"dim of leftover IntAlloy: \", np.sum(final_leftover_IntAlloy,axis=1,keepdims=True).shape,\"\\n\"\n",
    "    #print \"IntAlloy utilization ratio (utilized/leftover): \",final_utilized_IntAlloy/final_leftover_IntAlloy,\"\\n\"\n",
    "    #see percentage of intermediate scrap alloy utlized for alloy production \n",
    "    #print (\"IntAlloy utilization percent (utilized/(utilized+leftover) by vehicle type): \", final_utilized_ratio,\"\\n\") #(y,c,t)\n",
    "    #see percentage of alloy demand offset by using secondary material (with virgin bulk and AE)\n",
    "    #print \"Offset by secondary-derived alloys (offset/(offset+still_needed): \",final_virgin_alloy_offset/(final_virgin_alloy_offset+final_virgin_alloy_needed),\"\\n\"\n",
    "    \n",
    "    ###emissions\n",
    "    #print \"normalized LC emissions (kg CO2-eq/veh): \",final_norm_LC_emissions,\"\\n\"\n",
    "    \n",
    "    ###energy consumption\n",
    "    #print \"normalized LC energy (MJ/veh): \",final_norm_LC_energy,\"\\n\"\n",
    "    \n",
    "    \"\"\"export results\"\"\"\n",
    "    ###export scrap reuse ratios, reshaped to (y*c, t)    \n",
    "    final_utilized_ratio_reshaped=final_utilized_ratio.reshape((-1,final_utilized_ratio.shape[2])) #(y*c, t), no need to do \"order='F' \"\n",
    "    np.savetxt(\"scrap utilization ratios_CSV.csv\", final_utilized_ratio_reshaped, delimiter=\",\")\n",
    "    \n",
    "    ###export energy consumption, reshape to (x*c,t)\n",
    "    final_norm_LC_energy_reshaped=final_norm_LC_energy.reshape((-1,final_norm_LC_energy.shape[2])) #(x*c,t), if no [min,max] combination is investigated, x (axis=0) should be 1\n",
    "    np.savetxt(\"normalized LC energy (MJ per veh)_CSV.csv\",final_norm_LC_energy_reshaped, delimiter=\",\")\n",
    "    \n",
    "    \n",
    "    ###export GHG emissions, reshape to (x*c,t)\n",
    "    final_norm_LC_emissions_reshaped=final_norm_LC_emissions.reshape((-1,final_norm_LC_emissions.shape[2])) #(x*c,t), if no [min,max] combination is investigated, x (axis=0) should be 1\n",
    "    np.savetxt(\"normalized LC emissions (kg CO2-eq per veh)_CSV.csv\",final_norm_LC_emissions_reshaped,delimiter=\",\")\n",
    "    \n",
    "       \n",
    "    return final_utilized_ratio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test the Wrapper module\"\"\"\n",
    "##====dummy data====##\n",
    "#v_c_tensor_dummy=np.ones((4,1,3))*1000 #4 components, 3 vehicle types [now an intermediate variable converted from Data parsing]\n",
    "v_s_tensor_dummy=np.ones((7,1,6)) #dummy data, but REAL DIMENSIONS: 7 components, 6 vehicles\n",
    "#mat_compnt_tensor_dummy=np.ones((5,4,3))*2 #5 alloys, 4 components, 3 vehicles [now an intermediate variable output from Data parsing]\n",
    "#AE_matrix_dummy=np.arange(360).reshape((60,6)) #60 alloys, 12 AE types [now an intermediate variable output from Data parsing]\n",
    "shredding_loss_rate_dummy=[0.075,0.075,0.075,0.075,0.075,0.075,0.075] #REAL data,REAL DIMENSIONS: 7 components\n",
    "#shredding_loss_rate_dummy=[0.0,0.0,0.0,0.0,0.0,0.0,0.0] #TEST no shredding loss\n",
    "alloy_cutoff_dummy=[(0,3),(4,5),(6,57),(58,59)] #THIS IS REAL DATA FOR ALLOY (BY METAL GROUP) CUTOFF in alloy-related tensors (dim_nc)\n",
    "                                                #[steel,Fe,Al,Mg] sequence for alloy by metal group\n",
    "#TD_matrix_dummy=np.linspace(0.1,1.0,48).reshape(4,12) #dummy data, but REAL DIMENSIONS: 4 metal groups, 12 AE types\n",
    "#TargetAlloy_mass_by_compnt_dummy=np.random.rand(5,6,4)*10000 #5 alloys, 6 AE types, 4 components [now an intermediate variable converted from Data par\n",
    "compnt_loc_tuple_list=[(2,61), (63,122),(124,183),(185,244),(246,305),(307,366),(368,427)] #THIS IS REAL DATA for vehicle composition\n",
    "MFA_time_series_file='Vehicle flows_input_for_VehiReLCA_beta1.2.xlsx'\n",
    "scenario_num=2\n",
    "metal_process_data_workbook='Data_input_for_VehiReLCA_beta1.2.xlsx'\n",
    "\n",
    "body_composition_workbook='Composition_input_for_VehiReLCA_beta1.2.xlsx'\n",
    "time_span_tuple=(2016,2050)\n",
    "#Depreciated, bulk_metal_row_list=[142,142,140,145] #REAL data for bulk metals made from primary source [steel,Fe,Al,Mg]\n",
    "                       #CAUTION: 1st and 2nd elements of this list both refer to 'iron primary', as steel group (1st element) also requires iron as bulk metal\n",
    "\"\"\"ALSO NEED TO DEAL WITH two Mg primary sources!!! (need for both bulk and AE)\"\"\"\n",
    "#solution: assign all demand to the row of \"Electrolytic\" first, then use an exogenous variable to allocate part of the demand to 'Pigdeon'\n",
    "    \n",
    "AE_row_list=[140,141,142,143,144,145,147,148,149,150,151,152] #REAL data for AE, make sure the sequence of row# in this list matches the sequence of AE in columns\n",
    "                                #[Al,Si,Fe,Cu,Mn,Mg,Cr,Ni,Zn,Ti,Mo,C]\n",
    "Mg_Pigd_share_dict={146:0.8} #dummy data for 'Pigdeon' share, but REAL row#\n",
    "\n",
    "remelt_scrap_row_list=[153,156,154,155] #REAL data for remelted scrap, make sure the sequence of row# in this list matches the sequence of scrap remelted in PIOT\n",
    "                                        #[steel,Fe,Al,Mg] sequence for scrap remelted\n",
    "\n",
    "#input variables for new optimization-based blending func\n",
    "dummy_alpha=15 #dummy data, seems to work well (at least in testing of the func itself)\n",
    "dummy_num_iter=100 #dummy data, seems to work well (at least in testing of the func itself)\n",
    "dummy_epsilon=0.005 #dummy data, seems to work well (at least in testing of the func itself)\n",
    "dummy_total_utility_score_goal=0.01 #dummy data, seems to work well (at least in testing of the func itself)\n",
    "\n",
    "#input variable for closed-loop recycling rate factors\n",
    "#closed_loop_recycl_rate_list=[0.5,1,0.5,1] #dummy data for closed-loop recycling rate factors\n",
    "#closed_loop_recycl_rate_list=[0,0,0,0] #test zero utilization case\n",
    "closed_loop_recycl_rate_list=[0.26,1,0.75,0.52] #GREET data for closed-loop recycling rate factors\n",
    "                                             #0.26 for steel, 1 (assumed) for iron, 0.75 (assume average between wrought and cast Al), 0.52 for Mg\n",
    "\n",
    "    \n",
    "#kwargs\n",
    "assembly_energy_consumption_MJ_vehicle=8340 #value from GREET2 'Vehi_inputs' tab, MJ/vehi\n",
    "assembly_emissions_kgCO2eq_vehicle=0.127*8340 #assume 100% electricity-powered assemblying process, EF of electricity is the same as what's used for other LCA steps\n",
    "    \n",
    "\"\"\"test run (cross-mixing)\n",
    "final_utilized_ratio=Wrapper_MOD(MFA_time_series_file,scenario_num,metal_process_data_workbook,body_composition_workbook,time_span_tuple,compnt_loc_tuple_list,\n",
    "                v_s_tensor_dummy,shredding_loss_rate_dummy,\n",
    "            alloy_cutoff_dummy,bulk_metal_row_list,AE_row_list,\n",
    "            Mg_Pigd_share_dict,remelt_scrap_row_list,dummy_alpha,dummy_num_iter,dummy_epsilon,dummy_total_utility_score_goal,closed_loop_recycl_rate_list,\n",
    "            max_or_min='min',dilution_explicit=True,rand_sort_error=0.0,cross_mixing_flag=True,assembly_energy_consumption_MJ_vehicle=0.008,\n",
    "            assembly_emissions_kgCO2eq_vehicle=0.127*0.008)\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"test run (no cross-mixing)\"\"\"\n",
    "final_utilized_ratio=Wrapper_MOD(MFA_time_series_file,scenario_num,metal_process_data_workbook,body_composition_workbook,time_span_tuple,compnt_loc_tuple_list,\n",
    "                v_s_tensor_dummy,shredding_loss_rate_dummy,\n",
    "            alloy_cutoff_dummy,AE_row_list,\n",
    "            Mg_Pigd_share_dict,remelt_scrap_row_list,dummy_alpha,dummy_num_iter,dummy_epsilon,dummy_total_utility_score_goal,closed_loop_recycl_rate_list,\n",
    "            max_or_min='min',dilution_explicit=True,rand_sort_error=0.02,\n",
    "                cross_mixing_flag=True,assembly_energy_consumption_MJ_vehicle=8340,\n",
    "            assembly_emissions_kgCO2eq_vehicle=0.127*8340)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
